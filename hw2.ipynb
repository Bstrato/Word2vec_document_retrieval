{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bstrato/Word2vec_document_retrieval/blob/main/hw2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fl_zYOvskXH5"
      },
      "source": [
        "# HW2 Overview\n",
        "\n",
        "\n",
        "In this assignment, we will study neural IR, including word embedding and LLM.\n",
        "\n",
        "We will reuse the same Yelp dataset and refer to each individual user review as a **document**. You should reuse your JSON parser in this assignment.\n",
        "\n",
        "The same pre-processing steps you have developed in HW1 will be used in this assignment, i.e., tokenization, stemming, normalization and stopword removal.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRCnDCIAkXH7"
      },
      "source": [
        "# Word Embedding (50 points)\n",
        "\n",
        "Use [genism](https://radimrehurek.com/gensim/models/word2vec.html) library, download a pre-trained model or train a word2vec model (10 epochs) using the review data. Then use the model to get the vector representations of the document and query.\n",
        "\n",
        "Hint: You can average the embedding of the terms to get the vector representation\n",
        "for a document.\n",
        "\n",
        "Use the following 10 queries (same as the ones in HW1) and retrieve the top 3 documents for each query based on cosine similarity:\n",
        "\n",
        "\tgeneral chicken\n",
        "\tfried chicken\n",
        "\tBBQ sandwiches\n",
        "\tmashed potatoes\n",
        "\tGrilled Shrimp Salad\n",
        "\tlamb Shank\n",
        "\tPepperoni pizza\n",
        "\tbrussel sprout salad\n",
        "\tFRIENDLY STAFF\n",
        "\tGrilled Cheese\n",
        "\n",
        "Note that the training does not require GPU -- moderate CPU is good enough.  If your computation power is limited, i.e., limited memory or cpu, you can choose to train the model on partial data, e.g., 50% or fewer review data. Please document the corresponding training detail in your report.\n",
        "\n",
        "**What to submit**:\n",
        "\n",
        "1. Paste your implementation of training document representation and cosine similarity calculation. Report the training time of the word embedding model. (15 points)\n",
        "2. For the top 3 documents of each query, print the document and its cosine similarity score. (15 points)\n",
        "3. For the first three queries, analyze the relation between relevance and cosine similarity score: is a high score document more relevant to the query? (10 points)\n",
        "4. Are the documents more relevant than documents retrieved by TF-IDF in Homework 1? Why or why not? Compare and discuss the results. (10 points)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-6JYin2kXH8"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "import gensim.downloader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade gensim numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnOZtVmF8Nr8",
        "outputId": "a32171a8-6e29-434e-bb38-0498cdb27690"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Collecting numpy\n",
            "  Downloading numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import nltk\n",
        "import string\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import time\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Try to import gensim, with fallback to manual Word2Vec implementation\n",
        "try:\n",
        "    from gensim.models import Word2Vec\n",
        "    GENSIM_AVAILABLE = True\n",
        "    print(\"Using Gensim Word2Vec\")\n",
        "except (ImportError, ValueError) as e:\n",
        "    print(f\"Gensim import failed: {e}\")\n",
        "    print(\"Using fallback Word2Vec implementation\")\n",
        "    GENSIM_AVAILABLE = False\n",
        "\n",
        "# Try to import sklearn, with fallback to manual cosine similarity\n",
        "try:\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "    SKLEARN_AVAILABLE = True\n",
        "except ImportError:\n",
        "    SKLEARN_AVAILABLE = False\n",
        "    print(\"sklearn not available, using manual cosine similarity\")\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Initialize preprocessing tools\n",
        "stop_words = set(stopwords.words('english'))\n",
        "punctuations = set(string.punctuation)\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "\n",
        "def normalize_token(token):\n",
        "    \"\"\"Normalize a token by converting to lowercase and handling special cases.\"\"\"\n",
        "    token = token.lower()\n",
        "    if token in punctuations:\n",
        "        return None\n",
        "    if token.replace('.', '', 1).isdigit():\n",
        "        return 'NUM'\n",
        "    return token\n",
        "\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Preprocess text by tokenizing, normalizing, removing stopwords, and stemming.\"\"\"\n",
        "    if not text or not isinstance(text, str):\n",
        "        return []\n",
        "\n",
        "    try:\n",
        "        tokens = word_tokenize(text)\n",
        "        processed = []\n",
        "        for token in tokens:\n",
        "            norm = normalize_token(token)\n",
        "            if norm is None or norm in stop_words:\n",
        "                continue\n",
        "            stemmed = stemmer.stem(norm)\n",
        "            processed.append(stemmed)\n",
        "        return processed\n",
        "    except LookupError as e:\n",
        "        # Fallback to simple tokenization\n",
        "        simple_tokens = text.split()\n",
        "        processed = []\n",
        "        for token in simple_tokens:\n",
        "            norm = normalize_token(token)\n",
        "            if norm is None or norm in stop_words:\n",
        "                continue\n",
        "            stemmed = stemmer.stem(norm)\n",
        "            processed.append(stemmed)\n",
        "        return processed\n",
        "\n",
        "\n",
        "def manual_cosine_similarity(vec1, vec2):\n",
        "    \"\"\"Manual implementation of cosine similarity.\"\"\"\n",
        "    dot_product = np.dot(vec1, vec2)\n",
        "    norm1 = np.linalg.norm(vec1)\n",
        "    norm2 = np.linalg.norm(vec2)\n",
        "\n",
        "    if norm1 == 0 or norm2 == 0:\n",
        "        return 0.0\n",
        "\n",
        "    return dot_product / (norm1 * norm2)\n",
        "\n",
        "\n",
        "class SimpleWord2Vec:\n",
        "    \"\"\"\n",
        "    Simplified Word2Vec implementation using skip-gram with negative sampling.\n",
        "    This is a fallback when Gensim is not available.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vector_size=100, window=5, min_count=2, epochs=10, learning_rate=0.025):\n",
        "        self.vector_size = vector_size\n",
        "        self.window = window\n",
        "        self.min_count = min_count\n",
        "        self.epochs = epochs\n",
        "        self.learning_rate = learning_rate\n",
        "        self.vocab = {}\n",
        "        self.word_vectors = {}\n",
        "        self.vocab_size = 0\n",
        "\n",
        "    def build_vocab(self, sentences):\n",
        "        \"\"\"Build vocabulary from sentences.\"\"\"\n",
        "        word_counts = {}\n",
        "\n",
        "        # Count word frequencies\n",
        "        for sentence in sentences:\n",
        "            for word in sentence:\n",
        "                word_counts[word] = word_counts.get(word, 0) + 1\n",
        "\n",
        "        # Filter by min_count\n",
        "        self.vocab = {word: idx for idx, (word, count) in enumerate(word_counts.items())\n",
        "                     if count >= self.min_count}\n",
        "        self.vocab_size = len(self.vocab)\n",
        "\n",
        "        # Initialize random word vectors\n",
        "        np.random.seed(42)\n",
        "        for word in self.vocab:\n",
        "            self.word_vectors[word] = np.random.uniform(-0.5, 0.5, self.vector_size)\n",
        "\n",
        "        print(f\"Built vocabulary with {self.vocab_size} words\")\n",
        "\n",
        "    def train(self, sentences):\n",
        "        \"\"\"Simple training using random walks.\"\"\"\n",
        "        self.build_vocab(sentences)\n",
        "\n",
        "        print(f\"Training for {self.epochs} epochs...\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            if epoch % 2 == 0:\n",
        "                print(f\"Epoch {epoch + 1}/{self.epochs}\")\n",
        "\n",
        "            for sentence in sentences:\n",
        "                sentence_words = [word for word in sentence if word in self.vocab]\n",
        "\n",
        "                for i, target_word in enumerate(sentence_words):\n",
        "                    # Get context words within window\n",
        "                    start = max(0, i - self.window)\n",
        "                    end = min(len(sentence_words), i + self.window + 1)\n",
        "\n",
        "                    for j in range(start, end):\n",
        "                        if i != j:\n",
        "                            context_word = sentence_words[j]\n",
        "                            # Simple update rule (simplified skip-gram)\n",
        "                            self._update_vectors(target_word, context_word)\n",
        "\n",
        "        training_time = time.time() - start_time\n",
        "        print(f\"Training completed in {training_time:.2f} seconds ({training_time/60:.2f} minutes)\")\n",
        "        return training_time\n",
        "\n",
        "    def _update_vectors(self, target_word, context_word):\n",
        "        \"\"\"Simple vector update rule.\"\"\"\n",
        "        if target_word in self.word_vectors and context_word in self.word_vectors:\n",
        "            # Simplified gradient update\n",
        "            target_vec = self.word_vectors[target_word]\n",
        "            context_vec = self.word_vectors[context_word]\n",
        "\n",
        "            # Simple averaging update\n",
        "            update = self.learning_rate * (context_vec - target_vec) * 0.1\n",
        "            self.word_vectors[target_word] += update\n",
        "\n",
        "    def get_vector(self, word):\n",
        "        \"\"\"Get vector for a word.\"\"\"\n",
        "        return self.word_vectors.get(word, np.zeros(self.vector_size))\n",
        "\n",
        "    def __contains__(self, word):\n",
        "        \"\"\"Check if word is in vocabulary.\"\"\"\n",
        "        return word in self.word_vectors\n",
        "\n",
        "\n",
        "def get_document_vector(tokens, model, vector_size=100):\n",
        "    \"\"\"\n",
        "    Get document vector by averaging word vectors of all tokens in the document.\n",
        "    Works with both Gensim and SimpleWord2Vec models.\n",
        "    \"\"\"\n",
        "    vectors = []\n",
        "\n",
        "    if GENSIM_AVAILABLE and hasattr(model, 'wv'):\n",
        "        # Gensim model\n",
        "        for token in tokens:\n",
        "            if token in model.wv:\n",
        "                vectors.append(model.wv[token])\n",
        "    else:\n",
        "        # SimpleWord2Vec model\n",
        "        for token in tokens:\n",
        "            if token in model:\n",
        "                vectors.append(model.get_vector(token))\n",
        "\n",
        "    if vectors:\n",
        "        return np.mean(vectors, axis=0)\n",
        "    else:\n",
        "        # Return zero vector if no words found in vocabulary\n",
        "        return np.zeros(vector_size)\n",
        "\n",
        "\n",
        "def calculate_cosine_similarity(vec1, vec2):\n",
        "    \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n",
        "    if SKLEARN_AVAILABLE:\n",
        "        # Use sklearn implementation\n",
        "        vec1 = vec1.reshape(1, -1)\n",
        "        vec2 = vec2.reshape(1, -1)\n",
        "        return cosine_similarity(vec1, vec2)[0][0]\n",
        "    else:\n",
        "        # Use manual implementation\n",
        "        return manual_cosine_similarity(vec1, vec2)\n",
        "\n",
        "\n",
        "def word2vec_document_retrieval(yelp_folder=\"/content/sample_data/yelp\"):\n",
        "    \"\"\"\n",
        "    Main function to train Word2Vec model and perform document retrieval.\n",
        "    \"\"\"\n",
        "    json_files = glob.glob(os.path.join(yelp_folder, \"*.json\"))\n",
        "\n",
        "    if not json_files:\n",
        "        print(f\"No JSON files found in {yelp_folder}\")\n",
        "        return\n",
        "\n",
        "    print(f\"Found {len(json_files)} JSON files in {yelp_folder}\")\n",
        "\n",
        "    # Data structures for processing\n",
        "    documents = []  # Store original documents with metadata\n",
        "    processed_documents = []  # Store processed tokens for each document\n",
        "    total_reviews = 0\n",
        "\n",
        "    # Process all JSON files\n",
        "    for file_path in json_files:\n",
        "        print(f\"\\nProcessing file: {os.path.basename(file_path)}\")\n",
        "\n",
        "        try:\n",
        "            with open(file_path, 'r') as f:\n",
        "                try:\n",
        "                    data = json.load(f)\n",
        "                except json.JSONDecodeError:\n",
        "                    f.seek(0)\n",
        "                    data = []\n",
        "                    for line in f:\n",
        "                        if line.strip():\n",
        "                            data.append(json.loads(line.strip()))\n",
        "\n",
        "            # Extract reviews\n",
        "            reviews = []\n",
        "            if isinstance(data, dict) and 'Reviews' in data:\n",
        "                reviews = data['Reviews']\n",
        "                print(f\"Found {len(reviews)} reviews in a single JSON object\")\n",
        "            elif isinstance(data, list):\n",
        "                for obj in data:\n",
        "                    if isinstance(obj, dict) and 'Reviews' in obj:\n",
        "                        reviews.extend(obj['Reviews'])\n",
        "                print(f\"Found {len(reviews)} total reviews across multiple objects\")\n",
        "\n",
        "            if not reviews:\n",
        "                print(\"No reviews found in this file!\")\n",
        "                continue\n",
        "\n",
        "            # Find review text field and review ID field\n",
        "            potential_text_fields = ['Content', 'ReviewText', 'Text', 'review_text', 'comment', 'content', 'review']\n",
        "            potential_id_fields = ['ReviewID', 'ReviewId', 'review_id', 'id', 'Id', 'ID', 'reviewId', 'Review_ID']\n",
        "\n",
        "            review_field_name = None\n",
        "            review_id_field = None\n",
        "\n",
        "            # Find review text field\n",
        "            for field in potential_text_fields:\n",
        "                if field in reviews[0]:\n",
        "                    review_field_name = field\n",
        "                    print(f\"Found review text in field: '{field}'\")\n",
        "                    break\n",
        "\n",
        "            # Find review ID field\n",
        "            for field in potential_id_fields:\n",
        "                if field in reviews[0]:\n",
        "                    review_id_field = field\n",
        "                    print(f\"Found review ID in field: '{field}'\")\n",
        "                    break\n",
        "\n",
        "            if not review_field_name:\n",
        "                print(\"Could not identify review text field. Available fields:\")\n",
        "                print(list(reviews[0].keys()) if reviews else \"No reviews found\")\n",
        "                continue\n",
        "\n",
        "            if not review_id_field:\n",
        "                print(\"Could not identify review ID field. Will use generated IDs.\")\n",
        "                print(\"Available fields:\", list(reviews[0].keys()) if reviews else \"No reviews found\")\n",
        "\n",
        "            # Process each review\n",
        "            for i, review in enumerate(reviews):\n",
        "                total_reviews += 1\n",
        "                text = review.get(review_field_name, '')\n",
        "                tokens = preprocess_text(text)\n",
        "\n",
        "                if tokens:  # Only add non-empty documents\n",
        "                    processed_documents.append(tokens)\n",
        "\n",
        "                    # Use actual review ID if available, otherwise generate one\n",
        "                    if review_id_field and review.get(review_id_field):\n",
        "                        review_id = str(review.get(review_id_field))\n",
        "                    else:\n",
        "                        review_id = f\"{os.path.basename(file_path)}_review_{i}\"\n",
        "\n",
        "                    documents.append({\n",
        "                        'id': review_id,\n",
        "                        'text': text,\n",
        "                        'tokens': tokens,\n",
        "                        'file': os.path.basename(file_path)\n",
        "                    })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing file {file_path}: {e}\")\n",
        "\n",
        "    print(f\"\\nProcessed {total_reviews} reviews in total\")\n",
        "    print(f\"Valid documents for retrieval: {len(documents)}\")\n",
        "\n",
        "    # Train Word2Vec model\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"TRAINING WORD2VEC MODEL\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    print(f\"Training Word2Vec model with {len(processed_documents)} documents...\")\n",
        "\n",
        "    training_start_time = time.time()\n",
        "\n",
        "    if GENSIM_AVAILABLE:\n",
        "        # Use Gensim Word2Vec\n",
        "        try:\n",
        "            model = Word2Vec(\n",
        "                sentences=processed_documents,\n",
        "                vector_size=100,\n",
        "                window=5,\n",
        "                min_count=2,\n",
        "                workers=4,\n",
        "                epochs=10,\n",
        "                sg=0  # CBOW algorithm\n",
        "            )\n",
        "            training_time = time.time() - training_start_time\n",
        "            print(f\"Gensim Word2Vec model trained successfully!\")\n",
        "            print(f\"Training time: {training_time:.2f} seconds ({training_time/60:.2f} minutes)\")\n",
        "            print(f\"Vocabulary size: {len(model.wv.key_to_index)}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Gensim training failed: {e}\")\n",
        "            print(\"Falling back to simple Word2Vec implementation...\")\n",
        "            model = SimpleWord2Vec(vector_size=100, window=5, min_count=2, epochs=10)\n",
        "            training_time = model.train(processed_documents)\n",
        "            print(f\"Simple Word2Vec model trained successfully!\")\n",
        "            print(f\"Vocabulary size: {model.vocab_size}\")\n",
        "    else:\n",
        "        # Use fallback implementation\n",
        "        model = SimpleWord2Vec(vector_size=100, window=5, min_count=2, epochs=10)\n",
        "        training_time = model.train(processed_documents)\n",
        "        print(f\"Simple Word2Vec model trained successfully!\")\n",
        "        print(f\"Vocabulary size: {model.vocab_size}\")\n",
        "\n",
        "    # Generate document vectors\n",
        "    print(\"\\nGenerating document vectors...\")\n",
        "    document_vectors = []\n",
        "    for doc in documents:\n",
        "        doc_vector = get_document_vector(doc['tokens'], model, vector_size=100)\n",
        "        document_vectors.append(doc_vector)\n",
        "\n",
        "    print(f\"Generated {len(document_vectors)} document vectors\")\n",
        "\n",
        "    # Document Retrieval\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"DOCUMENT RETRIEVAL\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    queries = [\n",
        "        \"general chicken\",\n",
        "        \"fried chicken\",\n",
        "        \"BBQ sandwiches\",\n",
        "        \"mashed potatoes\",\n",
        "        \"Grilled Shrimp Salad\",\n",
        "        \"lamb Shank\",\n",
        "        \"Pepperoni pizza\",\n",
        "        \"brussel sprout salad\",\n",
        "        \"FRIENDLY STAFF\",\n",
        "        \"Grilled Cheese\"\n",
        "    ]\n",
        "\n",
        "    all_results = {}\n",
        "\n",
        "    for query in queries:\n",
        "        print(f\"\\nQuery: '{query}'\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        # Preprocess query\n",
        "        query_tokens = preprocess_text(query)\n",
        "        print(f\"Processed query tokens: {query_tokens}\")\n",
        "\n",
        "        # Get query vector\n",
        "        query_vector = get_document_vector(query_tokens, model, vector_size=100)\n",
        "\n",
        "        # Calculate similarities with all documents\n",
        "        similarities = []\n",
        "        for i, doc_vector in enumerate(document_vectors):\n",
        "            similarity = calculate_cosine_similarity(query_vector, doc_vector)\n",
        "            similarities.append((i, similarity))\n",
        "\n",
        "        # Sort by similarity (descending)\n",
        "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Get top 3 results\n",
        "        top_3 = similarities[:3]\n",
        "        all_results[query] = []\n",
        "\n",
        "        print(\"Top 3 most similar documents:\")\n",
        "        for rank, (doc_idx, similarity) in enumerate(top_3, 1):\n",
        "            doc = documents[doc_idx]\n",
        "            print(f\"  {rank}. Similarity: {similarity:.4f}\")\n",
        "            print(f\"     Review ID: {doc['id']}\")\n",
        "            print(f\"     Text preview: {doc['text'][:200]}...\")\n",
        "            print()\n",
        "\n",
        "            all_results[query].append({\n",
        "                'rank': rank,\n",
        "                'review_id': doc['id'],\n",
        "                'similarity': similarity,\n",
        "                'text_preview': doc['text'][:200]\n",
        "            })\n",
        "\n",
        "    # Summary of results\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"RETRIEVAL RESULTS SUMMARY\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    for query, results in all_results.items():\n",
        "        print(f\"\\nQuery: '{query}'\")\n",
        "        for result in results:\n",
        "            print(f\"  {result['rank']}. {result['review_id']} (sim: {result['similarity']:.4f})\")\n",
        "\n",
        "    # Save Word2Vec model\n",
        "    try:\n",
        "        if GENSIM_AVAILABLE and hasattr(model, 'save'):\n",
        "            model.save(\"word2vec_model.model\")\n",
        "            print(f\"\\nGensim Word2Vec model saved as 'word2vec_model.model'\")\n",
        "        else:\n",
        "            # Save simple model using pickle or numpy\n",
        "            import pickle\n",
        "            with open(\"simple_word2vec_model.pkl\", \"wb\") as f:\n",
        "                pickle.dump(model, f)\n",
        "            print(f\"\\nSimple Word2Vec model saved as 'simple_word2vec_model.pkl'\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not save model: {e}\")\n",
        "\n",
        "    return {\n",
        "        'total_reviews': total_reviews,\n",
        "        'total_documents': len(documents),\n",
        "        'vocabulary_size': len(model.wv.key_to_index) if GENSIM_AVAILABLE and hasattr(model, 'wv') else model.vocab_size,\n",
        "        'training_time_seconds': training_time,\n",
        "        'training_time_minutes': training_time/60,\n",
        "        'retrieval_results': all_results,\n",
        "        'model': model\n",
        "    }\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    results = word2vec_document_retrieval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3N_-TkD-3NM6",
        "outputId": "c0936775-5186-4b29-add0-809666d62699"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gensim import failed: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
            "Using fallback Word2Vec implementation\n",
            "Found 60 JSON files in /content/sample_data/yelp\n",
            "\n",
            "Processing file: gQCn4Gv-4_UUUFwpo-zHvA.json\n",
            "Found 1848 reviews in a single JSON object\n",
            "Found review text in field: 'Content'\n",
            "Found review ID in field: 'ReviewID'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing file: 7lnOgOMd7zys3bFfWY_jIw.json\n",
            "Found 2171 reviews in a single JSON object\n",
            "Found review text in field: 'Content'\n",
            "Found review ID in field: 'ReviewID'\n",
            "\n",
            "Processing file: WU_dFObt9VxHMcS1Eu32iQ.json\n",
            "Found 2664 reviews in a single JSON object\n",
            "Found review text in field: 'Content'\n",
            "Found review ID in field: 'ReviewID'\n",
            "\n",
            "Processing file: dlDEuDIvZI6I0cGZy4jIYg.json\n",
            "Found 1045 reviews in a single JSON object\n",
            "Found review text in field: 'Content'\n",
            "Found review ID in field: 'ReviewID'\n",
            "\n",
            "Processing file: J_rNhtb144k_fW50UQ7_lg.json\n",
            "Found 1511 reviews in a single JSON object\n",
            "Found review text in field: 'Content'\n",
            "Found review ID in field: 'ReviewID'\n",
            "\n",
            "Processing file: p8Uu-CYOUaISZT4w6OTNrQ.json\n",
            "Found 1583 reviews in a single JSON object\n",
            "Found review text in field: 'Content'\n",
            "Found review ID in field: 'ReviewID'\n",
            "\n",
            "Processing file: 3OLZOlqgOXdqY0uwxcOTfw.json\n",
            "Found 1951 reviews in a single JSON object\n",
            "Found review text in field: 'Content'\n",
            "Found review ID in field: 'ReviewID'\n",
            "\n",
            "Processing file: FtRrpU4mSRAkLNmLMGQ_Dg.json\n",
            "Found 1220 reviews in a single JSON object\n",
            "Found review text in field: 'Content'\n",
            "Found review ID in field: 'ReviewID'\n",
            "\n",
            "Processing file: 8y00IHK9sbA1Zhl2E9hmpA.json\n",
            "Found 1878 reviews in a single JSON object\n",
            "Found review text in field: 'Content'\n",
            "Found review ID in field: 'ReviewID'\n",
            "\n",
            "Processing file: A9xPHLcWtRgK6Mf4-ksBrw.json\n",
            "Found 1304 reviews in a single JSON object\n",
            "Found review text in field: 'Content'\n",
            "Found review ID in field: 'ReviewID'\n",
            "\n",
            "Processing file: rzHGofdCbaskvo2SzC6Vpg.json\n",
            "Found 1412 reviews in a single JSON object\n",
            "Found review text in field: 'Content'\n",
            "Found review ID in field: 'ReviewID'\n",
            "\n",
            "Processing file: SmYVU8I4ukBd1zJWdIxPcA.json\n",
            "Found 1478 reviews in a single JSON object\n",
            "Found review text in field: 'Content'\n",
            "Found review ID in field: 'ReviewID'\n",
            "\n",
            "Processing file: 9IRdWhDNo2T6vyMLwrQdMw.json\n",
            "Found 1843 reviews in a single JSON object\n",
            "Found review text in field: 'Content'\n",
            "Found review ID in field: 'ReviewID'\n",
            "\n",
            "Processing file: NQqSTW5c6bnsfVZaZAON1g.json\n",
            "Found 1406 reviews in a single JSON object\n",
            "Found review text in field: 'Content'\n",
            "Found review ID in field: 'ReviewID'\n",
            "\n",
            "Processing file: Q6nXAEw3UuAVFSztE4lPnA.json\n",
            "Found 1489 reviews in a single JSON object\n",
            "Found review text in field: 'Content'\n",
            "Found review ID in field: 'ReviewID'\n",
            "\n",
            "Processing file: lAPSCXomy_O3etWLxuQNUQ.json\n",
            "Found 1784 reviews in a single JSON object\n",
            "Found review text in field: 'Content'\n",
            "Found review ID in field: 'ReviewID'\n",
            "\n",
            "Processing file: AZrLjmV0A2TzOCkZ6CU-Fg.json\n",
            "Found 1474 reviews in a single JSON object\n",
            "Found review text in field: 'Content'\n",
            "Found review ID in field: 'ReviewID'\n",
            "\n",
            "Processing file: 9GVazUhx0dYB-_H0hlgB5w.json\n",
            "Found 1405 reviews in a single JSON object\n",
            "Found review text in field: 'Content'\n",
            "Found review ID in field: 'ReviewID'\n",
            "\n",
            "Processing file: VAi6XF1fbsx6LOjpLkPuTg.json\n",
            "Found 1889 reviews in a single JSON object\n",
            "Found review text in field: 'Content'\n",
            "Found review ID in field: 'ReviewID'\n",
            "\n",
            "Processing file: Z04-eU8EKLvnEsdX9ngqVQ.json\n",
            "Found 1629 reviews in a single JSON object\n",
            "Found review text in field: 'Content'\n",
            "Found review ID in field: 'ReviewID'\n",
            "\n",
            "Processing file: d5YWKrP-zG74nqOYzHn7Zg.json\n",
            "Found 2211 reviews in a single JSON object\n",
            "Found review text in field: 'Content'\n",
            "Found review ID in field: 'ReviewID'\n",
            "\n",
            "Processing file: Ud3xHfA2EP5BsmugEHwrhQ.json\n",
            "Found 1154 reviews in a single JSON object\n",
            "Found review text in field: 'Content'\n",
            "Found review ID in field: 'ReviewID'\n",
            "\n",
            "Processing file: 8d-RAa59tgV06VRa5SOe9A.json\n",
            "Found 1817 reviews in a single JSON object\n",
            "Found review text in field: 'Content'\n",
            "Found review ID in field: 'ReviewID'\n",
            "\n",
            "Processing file: 9N0YqwhE1qwU4OmhzwGjtA.json\n",
            "Found 1412 reviews in a single JSON object\n",
            "Found review text in field: 'Content'\n",
            "Found review ID in field: 'ReviewID'\n",
            "\n",
            "Processing file: y5EMn3YMd9tmUJ-guKS4eQ.json\n",
            "Found 1265 reviews in a single JSON object\n",
            "Found review text in field: 'Content'\n",
            "Found review ID in field: 'ReviewID'\n",
            "\n",
            "Processing file: tNnNr1AJf6ThLsSnPdd3xA.json\n",
            "Found 2185 reviews in a single JSON object\n",
            "Found review text in field: 'Content'\n",
            "Found review ID in field: 'ReviewID'\n",
            "\n",
            "Processing file: Nl8kv24kLtlGn5aqoggu8A.json\n",
            "Found 1912 reviews in a single JSON object\n",
            "Found review text in field: 'Content'\n",
            "Found review ID in field: 'ReviewID'\n",
            "\n",
            "Processing file: tEn9OKYKMVtVWiBP3NmcEg.json\n",
            "Found 1388 reviews in a single JSON object\n",
            "Found review text in field: 'Content'\n",
            "Found review ID in field: 'ReviewID'\n",
            "\n",
            "Processing file: t9wUtmbqYZ2jZf5GcbOKnQ.json\n",
            "Found 1313 reviews in a single JSON object\n",
            "Found review text in field: 'Content'\n",
            "Found review ID in field: 'ReviewID'\n",
            "\n",
            "Processing file: b7jDtbG7UloYasZ9sVXHWg.json\n",
            "Found 2195 reviews in a single JSON object\n",
            "Found review text in field: 'Content'\n",
            "Found review ID in field: 'ReviewID'\n",
            "\n",
            "Processing file: CvmAN25laBbwZTqmlEAR1Q.json\n",
            "Found 2614 reviews in a single JSON object\n",
            "Found review text in field: 'Content'\n",
            "Found review ID in field: 'ReviewID'\n",
            "\n",
            "Processing file: vTKSLe9DwRelou9wtcmHUA.json\n",
            "Found 1771 reviews in a single JSON object\n",
            "Found review text in field: 'Content'\n",
            "Found review ID in field: 'ReviewID'\n",
            "\n",
            "Processing file: z90atI98B7g1UvWn_i9ViQ.json\n",
            "Found 2011 reviews in a single JSON object\n",
            "Found review text in field: 'Content'\n",
            "Found review ID in field: 'ReviewID'\n",
            "\n",
            "Processing file: 4lgk5AJvmoXPrfSlCyjiQg.json\n",
            "Found 1138 reviews in a single JSON object\n",
            "Found review text in field: 'Content'\n",
            "Found review ID in field: 'ReviewID'\n",
            "\n",
            "Processing file: zkjcbVWPZHRzYdVzTIOA_Q.json\n",
            "Found 1355 reviews in a single JSON object\n",
            "Found review text in field: 'Content'\n",
            "Found review ID in field: 'ReviewID'\n",
            "\n",
            "Processing file: 3VCZ21-DIw7voVexDMXDSA.json\n",
            "Found 1915 reviews in a single JSON object\n",
            "Found review text in field: 'Content'\n",
            "Found review ID in field: 'ReviewID'\n",
            "\n",
            "Processing file: XJf0g9xKrMM-8uu1Mj4qsA.json\n",
            "Found 1388 reviews in a single JSON object\n",
            "Found review text in field: 'Content'\n",
            "Found review ID in field: 'ReviewID'\n",
            "\n",
            "Processing file: l2rPzn4wx2UhRaF5LKVJwA.json\n",
            "Found 2780 reviews in a single JSON object\n",
            "Found review text in field: 'Content'\n",
            "Found review ID in field: 'ReviewID'\n",
            "\n",
            "Processing file: pVEyB1BxiZkJUeoqHG3ehA.json\n",
            "Found 1154 reviews in a single JSON object\n",
            "Found review text in field: 'Content'\n",
            "Found review ID in field: 'ReviewID'\n",
            "\n",
            "Processing file: 7pezxyAK-7l5Yfr7W7fFQQ.json\n",
            "Found 1793 reviews in a single JSON object\n",
            "Found review text in field: 'Content'\n",
            "Found review ID in field: 'ReviewID'\n",
            "\n",
            "Processing file: EsypDkgdOEurddgPFyhCqw.json\n",
            "Found 2574 reviews in a single JSON object\n",
            "Found review text in field: 'Content'\n",
            "Found review ID in field: 'ReviewID'\n",
            "\n",
            "Processing file: tDpy78JYTqrClowdaJc6kA.json\n",
            "Found 1388 reviews in a single JSON object\n",
            "Found review text in field: 'Content'\n",
            "Found review ID in field: 'ReviewID'\n",
            "\n",
            "Processing file: dCw8uBtwnJFKwMnASlZLzg.json\n",
            "Found 2279 reviews in a single JSON object\n",
            "Found review text in field: 'Content'\n",
            "Found review ID in field: 'ReviewID'\n",
            "\n",
            "Processing file: 6CvMYmV0Uw4Y4G7NpvTgfg.json\n",
            "Found 1805 reviews in a single JSON object\n",
            "Found review text in field: 'Content'\n",
            "Found review ID in field: 'ReviewID'\n",
            "\n",
            "Processing file: UTMAQluppsu2ZDDaKKZyIQ.json\n",
            "Found 1301 reviews in a single JSON object\n",
            "Found review text in field: 'Content'\n",
            "Found review ID in field: 'ReviewID'\n",
            "\n",
            "Processing file: SlBFeVyO-ewdi6cC-fVVPA.json\n",
            "Found 2060 reviews in a single JSON object\n",
            "Found review text in field: 'Content'\n",
            "Found review ID in field: 'ReviewID'\n",
            "\n",
            "Processing file: aq_UTxlYvMNEN-urxbUddA.json\n",
            "Found 1677 reviews in a single JSON object\n",
            "Found review text in field: 'Content'\n",
            "Found review ID in field: 'ReviewID'\n",
            "\n",
            "Processing file: s46fIpN6IbmMxKKkyNGBGw.json\n",
            "Found 1113 reviews in a single JSON object\n",
            "Found review text in field: 'Content'\n",
            "Found review ID in field: 'ReviewID'\n",
            "\n",
            "Processing file: 0qnMSq3gYhGDesZKxNgAkQ.json\n",
            "Found 1445 reviews in a single JSON object\n",
            "Found review text in field: 'Content'\n",
            "Found review ID in field: 'ReviewID'\n",
            "\n",
            "Processing file: z5HiFhPj93NYrsrHJ86Wnw.json\n",
            "Found 1382 reviews in a single JSON object\n",
            "Found review text in field: 'Content'\n",
            "Found review ID in field: 'ReviewID'\n",
            "\n",
            "Processing file: Is7TtkCabINOEq7e-TiWXA.json\n",
            "Found 1741 reviews in a single JSON object\n",
            "Found review text in field: 'Content'\n",
            "Found review ID in field: 'ReviewID'\n",
            "\n",
            "Processing file: kO4SS2cYzFyUM04EUSGMpg.json\n",
            "Found 1094 reviews in a single JSON object\n",
            "Found review text in field: 'Content'\n",
            "Found review ID in field: 'ReviewID'\n",
            "\n",
            "Processing file: F_YQ3BrfHSMmKvJ_nqyYAA.json\n",
            "Found 2344 reviews in a single JSON object\n",
            "Found review text in field: 'Content'\n",
            "Found review ID in field: 'ReviewID'\n",
            "\n",
            "Processing file: okChDSotPCRtJlQVzlnw1Q.json\n",
            "Found 1954 reviews in a single JSON object\n",
            "Found review text in field: 'Content'\n",
            "Found review ID in field: 'ReviewID'\n",
            "\n",
            "Processing file: UKtzNIgmUd-9TljoI2wLiA.json\n",
            "Found 1920 reviews in a single JSON object\n",
            "Found review text in field: 'Content'\n",
            "Found review ID in field: 'ReviewID'\n",
            "\n",
            "Processing file: e1grgYnCxbue-tU5Gf6IlQ.json\n",
            "Found 1341 reviews in a single JSON object\n",
            "Found review text in field: 'Content'\n",
            "Found review ID in field: 'ReviewID'\n",
            "\n",
            "Processing file: ExDgGmuZJOsGXAzSIM1jtA.json\n",
            "Found 2203 reviews in a single JSON object\n",
            "Found review text in field: 'Content'\n",
            "Found review ID in field: 'ReviewID'\n",
            "\n",
            "Processing file: gB-0XpxMvsO04AMedpQs1Q.json\n",
            "Found 1654 reviews in a single JSON object\n",
            "Found review text in field: 'Content'\n",
            "Found review ID in field: 'ReviewID'\n",
            "\n",
            "Processing file: sWXT0sNodEnxZjziB3saZQ.json\n",
            "Found 2218 reviews in a single JSON object\n",
            "Found review text in field: 'Content'\n",
            "Found review ID in field: 'ReviewID'\n",
            "\n",
            "Processing file: zdzVyuYhpF5R2oiLuL0Rrw.json\n",
            "Found 953 reviews in a single JSON object\n",
            "Found review text in field: 'Content'\n",
            "Found review ID in field: 'ReviewID'\n",
            "\n",
            "Processed 102201 reviews in total\n",
            "Valid documents for retrieval: 102198\n",
            "\n",
            "==================================================\n",
            "TRAINING WORD2VEC MODEL\n",
            "==================================================\n",
            "Training Word2Vec model with 102198 documents...\n",
            "Built vocabulary with 94565 words\n",
            "Training for 10 epochs...\n",
            "Epoch 1/10\n",
            "Epoch 3/10\n",
            "Epoch 5/10\n",
            "Epoch 7/10\n",
            "Epoch 9/10\n",
            "Training completed in 2631.40 seconds (43.86 minutes)\n",
            "Simple Word2Vec model trained successfully!\n",
            "Vocabulary size: 94565\n",
            "\n",
            "Generating document vectors...\n",
            "Generated 102198 document vectors\n",
            "\n",
            "==================================================\n",
            "DOCUMENT RETRIEVAL\n",
            "==================================================\n",
            "\n",
            "Query: 'general chicken'\n",
            "----------------------------------------\n",
            "Processed query tokens: ['gener', 'chicken']\n",
            "Top 3 most similar documents:\n",
            "  1. Similarity: 0.9382\n",
            "     Review ID: JxxJmaPcIw6Y78-eXojSnA\n",
            "     Text preview: CHICKEN AND WAFFLES. Chicken and waffles. Chicken & Waffles.Anyway you put it; it is delicious....\n",
            "\n",
            "  2. Similarity: 0.9241\n",
            "     Review ID: NGUEsoOH3D88u_MtVHW7EA\n",
            "     Text preview: overhyped and left disappointed. the sauces were good but the chicken meat itself didn't taste very good....\n",
            "\n",
            "  3. Similarity: 0.9222\n",
            "     Review ID: 7_gq2hiNiBuVNOQhpPDyIQ\n",
            "     Text preview: My husband and I had the pleasure of eating here twice. The staff is amazing. They definitely make you feel at home. On our 1st visit we had the chicken and waffles and it was absolutely perfect! The ...\n",
            "\n",
            "\n",
            "Query: 'fried chicken'\n",
            "----------------------------------------\n",
            "Processed query tokens: ['fri', 'chicken']\n",
            "Top 3 most similar documents:\n",
            "  1. Similarity: 0.9543\n",
            "     Review ID: GjUngt4mXlk2dWECJEOOQA\n",
            "     Text preview: The Fried Chicken is amazing!!...\n",
            "\n",
            "  2. Similarity: 0.9244\n",
            "     Review ID: xrZ3o2_jUS038lQEYmow0Q\n",
            "     Text preview: I always have both the fried chicken and the waffle. Perhaps it's the excuse to put syrup on the chicken. Try the greens and you will have less guilt after eating the fried chicken and waffles....\n",
            "\n",
            "  3. Similarity: 0.9241\n",
            "     Review ID: Uzu1ZiXDpbPPzMAnSjvZQA\n",
            "     Text preview: Absolutely the worst fried chicken, service was also the worst............if you want fried chicken in NOLA go to KFC.....................\n",
            "\n",
            "\n",
            "Query: 'BBQ sandwiches'\n",
            "----------------------------------------\n",
            "Processed query tokens: ['bbq', 'sandwich']\n",
            "Top 3 most similar documents:\n",
            "  1. Similarity: 0.9109\n",
            "     Review ID: wsjt-de2rBJLypHlo8yU9A\n",
            "     Text preview: great bbq...\n",
            "\n",
            "  2. Similarity: 0.8784\n",
            "     Review ID: 6iiTX5o0F1wIYykz8GyltA\n",
            "     Text preview: Great BBQ in Chicago!...\n",
            "\n",
            "  3. Similarity: 0.8690\n",
            "     Review ID: BVwowMC4f9MV51YPFjzybw\n",
            "     Text preview: great authentic bbq will be back...\n",
            "\n",
            "\n",
            "Query: 'mashed potatoes'\n",
            "----------------------------------------\n",
            "Processed query tokens: ['mash', 'potato']\n",
            "Top 3 most similar documents:\n",
            "  1. Similarity: 0.8831\n",
            "     Review ID: dyjwK_6CnAi_nD2b9eqBZA\n",
            "     Text preview: Potato pizza...mmmmmmmmmmmmmmmm...\n",
            "\n",
            "  2. Similarity: 0.8661\n",
            "     Review ID: wn5D-25SbtVU7xMst0UqJQ\n",
            "     Text preview: I love that they put the giant sprig of rosemary on the mashed potatoes...\n",
            "\n",
            "  3. Similarity: 0.8651\n",
            "     Review ID: NZq405GHKq0jxfEciRyiuw\n",
            "     Text preview: Mashed potatoes on pizza? Sign me up!...\n",
            "\n",
            "\n",
            "Query: 'Grilled Shrimp Salad'\n",
            "----------------------------------------\n",
            "Processed query tokens: ['grill', 'shrimp', 'salad']\n",
            "Top 3 most similar documents:\n",
            "  1. Similarity: 0.8921\n",
            "     Review ID: BT9pZHFzj7LS8RxDxu5ZOw\n",
            "     Text preview: Great atmosphere! The sea bass looked good, but I ordered a salad with grilled shrimp. The vegetable salad was tasty. I think the lunch would be great....\n",
            "\n",
            "  2. Similarity: 0.8895\n",
            "     Review ID: SEKo_APq2jrBOr7uO_Hkhw\n",
            "     Text preview: Still going strong:seafood fideofrench toast and chicken stripschickpea saladcrispy shrimp cocktail...\n",
            "\n",
            "  3. Similarity: 0.8877\n",
            "     Review ID: yaC4M7xsl9pG8Wi4wuhfCA\n",
            "     Text preview: Always come to mothers when I visit NOLA. Had shrimp and oyster po-boy, red beans and rice,  grilled shrimp with turnip greens and sweet potato pie this time around. All excellent. If ordering shrimp ...\n",
            "\n",
            "\n",
            "Query: 'lamb Shank'\n",
            "----------------------------------------\n",
            "Processed query tokens: ['lamb', 'shank']\n",
            "Top 3 most similar documents:\n",
            "  1. Similarity: 0.8590\n",
            "     Review ID: 6-VphmQBLfEY-gLW040Hfw\n",
            "     Text preview: The lamb I order was smelly!!...\n",
            "\n",
            "  2. Similarity: 0.8117\n",
            "     Review ID: MLizHt3s4qZA_pxCw2Vk_A\n",
            "     Text preview: We ordered a bunch of small plates.  The Bacon Dates were amazing.  Lamb Skewers rocked.  Good wine.  Loved the Sangria.  Service was excellent....\n",
            "\n",
            "  3. Similarity: 0.8089\n",
            "     Review ID: Mvpg44rFJvS-yDderQY6YA\n",
            "     Text preview: I love the food.  You can tell that its all fresh and homemade.  Stuffed Peppers are awesome along with the Prawns and Leg of Lamb...\n",
            "\n",
            "\n",
            "Query: 'Pepperoni pizza'\n",
            "----------------------------------------\n",
            "Processed query tokens: ['pepperoni', 'pizza']\n",
            "Top 3 most similar documents:\n",
            "  1. Similarity: 0.9225\n",
            "     Review ID: KRQQsqdtM3cODL4t1g4r_w\n",
            "     Text preview: 6 wordsChicken BaconMashed potatoesBBQ Pizza...\n",
            "\n",
            "  2. Similarity: 0.9158\n",
            "     Review ID: rokoqIh9l3ykN6L0wCOVSA\n",
            "     Text preview: great pizza...\n",
            "\n",
            "  3. Similarity: 0.9125\n",
            "     Review ID: ONtFh9QP_sGVlmkeymSrmQ\n",
            "     Text preview: Sausage-fennell pizza is AMAZING!...\n",
            "\n",
            "\n",
            "Query: 'brussel sprout salad'\n",
            "----------------------------------------\n",
            "Processed query tokens: ['brussel', 'sprout', 'salad']\n",
            "Top 3 most similar documents:\n",
            "  1. Similarity: 0.9040\n",
            "     Review ID: Bpm31DykQaZ8-v2e7OEoYQ\n",
            "     Text preview: Yummy....get the brussel sprouts with brie and the golden beet salad with goat cheese!...\n",
            "\n",
            "  2. Similarity: 0.8731\n",
            "     Review ID: hZknLgl8JH1WtO884OSi-A\n",
            "     Text preview: almost the best steak i've ever had in my life. amazing brussel sprouts as well!...\n",
            "\n",
            "  3. Similarity: 0.8600\n",
            "     Review ID: p25fcZSPeFD3FsS0Eitilg\n",
            "     Text preview: For the most part, the food was good. Stay away from the brussel sprouts--very bitter....\n",
            "\n",
            "\n",
            "Query: 'FRIENDLY STAFF'\n",
            "----------------------------------------\n",
            "Processed query tokens: ['friendli', 'staff']\n",
            "Top 3 most similar documents:\n",
            "  1. Similarity: 0.9283\n",
            "     Review ID: xK-42JDZjWIZ0AnHA1K72Q\n",
            "     Text preview: Great food, reasonable prices, friendly staff...\n",
            "\n",
            "  2. Similarity: 0.9278\n",
            "     Review ID: hBBrw3iPuWT7lJJGE_vAfA\n",
            "     Text preview: Long wait good food friendly staff...\n",
            "\n",
            "  3. Similarity: 0.9173\n",
            "     Review ID: FkFwbwaWgdx8Lwzm5xJhuA\n",
            "     Text preview: Great place to eat with friends friendly staff and food is great...\n",
            "\n",
            "\n",
            "Query: 'Grilled Cheese'\n",
            "----------------------------------------\n",
            "Processed query tokens: ['grill', 'chees']\n",
            "Top 3 most similar documents:\n",
            "  1. Similarity: 0.8992\n",
            "     Review ID: KickyJibGLyWvyV_GTRkgg\n",
            "     Text preview: Grilled corn!...\n",
            "\n",
            "  2. Similarity: 0.8786\n",
            "     Review ID: QcsVNJqH777SwvffhCIN5Q\n",
            "     Text preview: great cheese and toppings...\n",
            "\n",
            "  3. Similarity: 0.8780\n",
            "     Review ID: 0FMc7RrnS3r23VjssBib0A\n",
            "     Text preview: The grilled oysters are very good....\n",
            "\n",
            "\n",
            "==================================================\n",
            "RETRIEVAL RESULTS SUMMARY\n",
            "==================================================\n",
            "\n",
            "Query: 'general chicken'\n",
            "  1. JxxJmaPcIw6Y78-eXojSnA (sim: 0.9382)\n",
            "  2. NGUEsoOH3D88u_MtVHW7EA (sim: 0.9241)\n",
            "  3. 7_gq2hiNiBuVNOQhpPDyIQ (sim: 0.9222)\n",
            "\n",
            "Query: 'fried chicken'\n",
            "  1. GjUngt4mXlk2dWECJEOOQA (sim: 0.9543)\n",
            "  2. xrZ3o2_jUS038lQEYmow0Q (sim: 0.9244)\n",
            "  3. Uzu1ZiXDpbPPzMAnSjvZQA (sim: 0.9241)\n",
            "\n",
            "Query: 'BBQ sandwiches'\n",
            "  1. wsjt-de2rBJLypHlo8yU9A (sim: 0.9109)\n",
            "  2. 6iiTX5o0F1wIYykz8GyltA (sim: 0.8784)\n",
            "  3. BVwowMC4f9MV51YPFjzybw (sim: 0.8690)\n",
            "\n",
            "Query: 'mashed potatoes'\n",
            "  1. dyjwK_6CnAi_nD2b9eqBZA (sim: 0.8831)\n",
            "  2. wn5D-25SbtVU7xMst0UqJQ (sim: 0.8661)\n",
            "  3. NZq405GHKq0jxfEciRyiuw (sim: 0.8651)\n",
            "\n",
            "Query: 'Grilled Shrimp Salad'\n",
            "  1. BT9pZHFzj7LS8RxDxu5ZOw (sim: 0.8921)\n",
            "  2. SEKo_APq2jrBOr7uO_Hkhw (sim: 0.8895)\n",
            "  3. yaC4M7xsl9pG8Wi4wuhfCA (sim: 0.8877)\n",
            "\n",
            "Query: 'lamb Shank'\n",
            "  1. 6-VphmQBLfEY-gLW040Hfw (sim: 0.8590)\n",
            "  2. MLizHt3s4qZA_pxCw2Vk_A (sim: 0.8117)\n",
            "  3. Mvpg44rFJvS-yDderQY6YA (sim: 0.8089)\n",
            "\n",
            "Query: 'Pepperoni pizza'\n",
            "  1. KRQQsqdtM3cODL4t1g4r_w (sim: 0.9225)\n",
            "  2. rokoqIh9l3ykN6L0wCOVSA (sim: 0.9158)\n",
            "  3. ONtFh9QP_sGVlmkeymSrmQ (sim: 0.9125)\n",
            "\n",
            "Query: 'brussel sprout salad'\n",
            "  1. Bpm31DykQaZ8-v2e7OEoYQ (sim: 0.9040)\n",
            "  2. hZknLgl8JH1WtO884OSi-A (sim: 0.8731)\n",
            "  3. p25fcZSPeFD3FsS0Eitilg (sim: 0.8600)\n",
            "\n",
            "Query: 'FRIENDLY STAFF'\n",
            "  1. xK-42JDZjWIZ0AnHA1K72Q (sim: 0.9283)\n",
            "  2. hBBrw3iPuWT7lJJGE_vAfA (sim: 0.9278)\n",
            "  3. FkFwbwaWgdx8Lwzm5xJhuA (sim: 0.9173)\n",
            "\n",
            "Query: 'Grilled Cheese'\n",
            "  1. KickyJibGLyWvyV_GTRkgg (sim: 0.8992)\n",
            "  2. QcsVNJqH777SwvffhCIN5Q (sim: 0.8786)\n",
            "  3. 0FMc7RrnS3r23VjssBib0A (sim: 0.8780)\n",
            "\n",
            "Simple Word2Vec model saved as 'simple_word2vec_model.pkl'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hg-LbKHSkXH9"
      },
      "source": [
        "## Retrieval with Large Language Models (LLMs) (40 points)\n",
        "\n",
        "This task introduces using LLMs for document retrieval through the free OpenRouter API. You will explore how to prompt an LLM to find relevant documents from the Yelp review dataset.\n",
        "\n",
        "**Setup:**\n",
        "1.  Sign up for a free account at [OpenRouter.ai](https://openrouter.ai/).\n",
        "2.  Obtain your API key from your OpenRouter dashboard.\n",
        "3.  Familiarize yourself with the [OpenRouter API Quickstart Guide](https://openrouter.ai/docs/quickstart), particularly how to make API calls using Python (the `openai` library is often used as OpenRouter mimics its API structure for many models).\n",
        "4.  Choose one of the free models available on OpenRouter for your experiments (e.g., Mistral 7B Instruct, Qwen 8B, etc.). Note the model you choose in your report.\n",
        "\n",
        "**Activities:**\n",
        "\n",
        "1.  **Introduction to LLM-based Retrieval (15 points)**\n",
        "    * Briefly explain (1 paragraph in your report) the concept of using LLMs for retrieval. What are potential advantages and disadvantages compared to traditional methods (TF-IDF) and embedding-based methods (Word2Vec/Doc2Vec)?\n",
        "    * Include a small code snippet that shows you can successfully connect to the OpenRouter API with your chosen model (e.g., a simple test query like \"What is the capital of France?\").\n",
        "\n",
        "2.  **Prompt Engineering for Document Retrieval (20 points)**\n",
        "    * Use the same 10 queries from previous tasks.\n",
        "    * **Strategy for presenting documents to the LLM:** Since LLM context windows are limited, you cannot pass all reviews for every query. You should devise a strategy. For example:\n",
        "        * **Option A (Re-ranking):** First, retrieve a larger set of candidate documents (e.g., top 5-10) for each query using one of your previous methods (TF-IDF, Word2Vec). Then, for each query, present these candidate documents (e.g., their content) to the LLM and ask it to select or re-rank the top 3 most relevant ones based on the query.\n",
        "        * **Option B (Direct Retrieval from a Subset):** For each query, randomly sample a small subset of reviews from the entire dataset. Present these to the LLM and ask it to identify the top 3 relevant to the query from this subset. (Repeat sampling if no relevant docs are found, or acknowledge this limitation).\n",
        "        * *Clearly describe the strategy you choose and why.*\n",
        "    * **Prompt Design:** Design 2-3 different prompt strategies to instruct the LLM. Examples:\n",
        "        * *Zero-shot Re-ranking:* \"Given the query '{query}' and the following reviews, which 3 reviews are most relevant? Provide only their IDs/numbers.\n",
        "Review 1: {review_content_1}\n",
        "Review 2: {review_content_2}\n",
        "...\"\n",
        "        * *Instruction-based Selection:* \"You are a helpful assistant. I am looking for restaurant reviews about '{query}'. From the list of reviews below, please identify the top 3 reviews that best match this topic. Focus on {specific aspect, e.g., food quality, specific dish mentioned}. Return the review text or ID.\n",
        "Reviews:\n",
        "{document_bundle}\"\n",
        "    * For each query, apply your chosen document presentation strategy and your *best performing* prompt strategy to retrieve the top 3 documents (reviews).\n",
        "    * **Deliverables for this part:**\n",
        "        * Clear documentation of your chosen document presentation strategy (Option A/B or other).\n",
        "        * The exact text of the 2-3 prompt strategies you designed.\n",
        "        * Python code snippets showing how you interacted with the OpenRouter API (formatting the prompt with the query and document data, making the call, parsing the response).\n",
        "        * The top 3 retrieved document IDs (or snippets) for each of the 10 queries using your best prompt.\n",
        "3. **Discussion and analysis (5 points)**\n",
        "    * Compare these LLM-retrieved results against those obtained from TF-IDF and Word2Vec. Which of your prompt strategies seemed to work best? Why do you think so?\n",
        "    * Discuss any challenges you faced (e.g., prompt sensitivity, LLM hallucinations, managing context length, API errors, cost/rate limits if applicable even on free tiers, parsing LLM output). What are the pros and cons of using LLMs for this retrieval task based on your experience?\n",
        "\n",
        "**What to submit:**\n",
        "\n",
        "* Your written explanation for the first question.\n",
        "* Your documented document presentation strategy, prompt designs, code for API interaction, and the top 3 retrieved documents per query.\n",
        "* Your discussion and analysis for question 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-wgvYllkXH-"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import os\n",
        "import json # For handling review data\n",
        "\n",
        "\n",
        "print(\"Task 3: Retrieval with LLMs via OpenRouter\")\n",
        "# --- Your code for Task 3 will go here ---\n",
        "\n",
        "# **Important:** Set your OpenRouter API key as an environment variable\n",
        "# or directly in the code (less secure, for testing only).\n",
        "# os.environ['OPENROUTER_API_KEY'] = \"YOUR_OPENROUTER_API_KEY\"\n",
        "# client = openai.OpenAI(\n",
        "#     base_url=\"https://openrouter.ai/api/v1\",\n",
        "#     api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
        "# )\n",
        "\n",
        "# Example: Test API connection\n",
        "# CHOSEN_LLM_MODEL = \"mistralai/mistral-7b-instruct:free\" # Example free model\n",
        "# try:\n",
        "#     completion = client.chat.completions.create(\n",
        "#         model=CHOSEN_LLM_MODEL,\n",
        "#         messages=[\n",
        "#             {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "#             {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n",
        "#         ],\n",
        "#         max_tokens=10,\n",
        "#         temperature=0.1\n",
        "#     )\n",
        "#     print(completion.choices[0].message.content)\n",
        "# except Exception as e:\n",
        "#     print(f\"Error connecting to OpenRouter: {e}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fi2GRaItZjY8",
        "outputId": "802fcdb0-e1fa-4881-addb-d9636eeba370"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.84.0-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Collecting jiter<1,>=0.4.0 (from openai)\n",
            "  Downloading jiter-0.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.13.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Downloading openai-1.84.0-py3-none-any.whl (725 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m725.5/725.5 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (352 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m352.2/352.2 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jiter, openai\n",
            "Successfully installed jiter-0.10.0 openai-1.84.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import time\n",
        "import re\n",
        "import glob\n",
        "import string\n",
        "import nltk\n",
        "import warnings\n",
        "from typing import List, Dict, Tuple\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Download required NLTK data\n",
        "try:\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "except:\n",
        "    print(\"Note: NLTK downloads may have failed, using fallback tokenization\")\n",
        "\n",
        "# Initialize preprocessing tools\n",
        "try:\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "except:\n",
        "    stop_words = {'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours'}\n",
        "\n",
        "punctuations = set(string.punctuation)\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def normalize_token(token):\n",
        "    \"\"\"Normalize a token by converting to lowercase and handling special cases.\"\"\"\n",
        "    token = token.lower()\n",
        "    if token in punctuations:\n",
        "        return None\n",
        "    if token.replace('.', '', 1).isdigit():\n",
        "        return 'NUM'\n",
        "    return token\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Preprocess text by tokenizing, normalizing, removing stopwords, and stemming.\"\"\"\n",
        "    if not text or not isinstance(text, str):\n",
        "        return []\n",
        "\n",
        "    try:\n",
        "        tokens = word_tokenize(text)\n",
        "        processed = []\n",
        "        for token in tokens:\n",
        "            norm = normalize_token(token)\n",
        "            if norm is None or norm in stop_words:\n",
        "                continue\n",
        "            stemmed = stemmer.stem(norm)\n",
        "            processed.append(stemmed)\n",
        "        return processed\n",
        "    except LookupError:\n",
        "        # Fallback to simple tokenization if NLTK data not available\n",
        "        simple_tokens = text.split()\n",
        "        processed = []\n",
        "        for token in simple_tokens:\n",
        "            norm = normalize_token(token)\n",
        "            if norm is None or norm in stop_words:\n",
        "                continue\n",
        "            stemmed = stemmer.stem(norm)\n",
        "            processed.append(stemmed)\n",
        "        return processed\n",
        "\n",
        "print(\"Task 3: Retrieval with LLMs via OpenRouter\")\n",
        "\n",
        "# **IMPORTANT:** Set your OpenRouter API key here\n",
        "# Replace \"YOUR_API_KEY_HERE\" with your actual OpenRouter API key\n",
        "os.environ['OPENROUTER_API_KEY'] = \"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\"\n",
        "\n",
        "# Initialize OpenRouter client\n",
        "client = openai.OpenAI(\n",
        "    base_url=\"https://openrouter.ai/api/v1\",\n",
        "    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
        ")\n",
        "\n",
        "# Choose a free model\n",
        "CHOSEN_LLM_MODEL = \"qwen/qwen3-8b:free\"\n",
        "\n",
        "# Test API connection first\n",
        "def test_api_connection():\n",
        "    \"\"\"Test the OpenRouter API connection\"\"\"\n",
        "    print(\"\\n=== Testing API Connection ===\")\n",
        "    try:\n",
        "        completion = client.chat.completions.create(\n",
        "            model=CHOSEN_LLM_MODEL,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n",
        "            ],\n",
        "            max_tokens=10,\n",
        "            temperature=0.1\n",
        "        )\n",
        "        print(f\"✓ API Connection Successful!\")\n",
        "        print(f\"Test Response: {completion.choices[0].message.content}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Error connecting to OpenRouter: {e}\")\n",
        "        return False\n",
        "\n",
        "# Load Yelp reviews data\n",
        "def find_yelp_files(directory_path: str) -> List[str]:\n",
        "    \"\"\"Find Yelp data files in the directory\"\"\"\n",
        "\n",
        "    print(f\"\\n=== Scanning directory: {directory_path} ===\")\n",
        "\n",
        "    # Common Yelp file patterns\n",
        "    patterns = [\n",
        "        \"yelp_academic_dataset_review.json\",\n",
        "        \"review.json\",\n",
        "        \"*review*.json\",\n",
        "        \"*.json\"\n",
        "    ]\n",
        "\n",
        "    found_files = []\n",
        "    for pattern in patterns:\n",
        "        full_pattern = os.path.join(directory_path, pattern)\n",
        "        matches = glob.glob(full_pattern)\n",
        "        if matches:\n",
        "            found_files.extend(matches)\n",
        "            break  # Use first successful pattern\n",
        "\n",
        "    if found_files:\n",
        "        print(f\"Found files: {found_files}\")\n",
        "    else:\n",
        "        print(f\"No JSON files found in {directory_path}\")\n",
        "        # List all files in directory for debugging\n",
        "        try:\n",
        "            all_files = os.listdir(directory_path)\n",
        "            print(f\"All files in directory: {all_files}\")\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    return found_files\n",
        "\n",
        "def load_yelp_data(file_path: str) -> List[Dict]:\n",
        "    \"\"\"Load Yelp reviews from JSON file with robust parsing\"\"\"\n",
        "    print(f\"\\n=== Loading Yelp Data from {file_path} ===\")\n",
        "\n",
        "    # If path is a directory, try to find the review file\n",
        "    if os.path.isdir(file_path):\n",
        "        print(f\"Path is a directory. Searching for review files...\")\n",
        "        found_files = find_yelp_files(file_path)\n",
        "        if not found_files:\n",
        "            return []\n",
        "        file_path = found_files[0]  # Use first found file\n",
        "        print(f\"Using file: {file_path}\")\n",
        "\n",
        "    try:\n",
        "        documents = []\n",
        "        total_reviews = 0\n",
        "\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            # Try to parse as single JSON first\n",
        "            try:\n",
        "                f.seek(0)\n",
        "                data = json.load(f)\n",
        "\n",
        "                # Handle different JSON structures\n",
        "                reviews = []\n",
        "                if isinstance(data, dict) and 'Reviews' in data:\n",
        "                    reviews = data['Reviews']\n",
        "                    print(f\"Found {len(reviews)} reviews in a single JSON object\")\n",
        "                elif isinstance(data, list):\n",
        "                    for obj in data:\n",
        "                        if isinstance(obj, dict):\n",
        "                            if 'Reviews' in obj:\n",
        "                                reviews.extend(obj['Reviews'])\n",
        "                            elif 'text' in obj or 'review_text' in obj:  # Direct review objects\n",
        "                                reviews.append(obj)\n",
        "                    print(f\"Found {len(reviews)} total reviews\")\n",
        "                elif isinstance(data, dict) and ('text' in data or 'review_text' in data):\n",
        "                    reviews = [data]  # Single review object\n",
        "\n",
        "                if not reviews:\n",
        "                    print(\"No reviews found in JSON structure!\")\n",
        "                    return []\n",
        "\n",
        "                # Process reviews with flexible field detection\n",
        "                potential_text_fields = ['text', 'Content', 'ReviewText', 'Text', 'review_text', 'comment', 'content', 'review']\n",
        "                potential_id_fields = ['review_id', 'ReviewID', 'ReviewId', 'id', 'Id', 'ID', 'reviewId', 'Review_ID']\n",
        "\n",
        "                review_field_name = None\n",
        "                review_id_field = None\n",
        "\n",
        "                # Find review text field\n",
        "                for field in potential_text_fields:\n",
        "                    if field in reviews[0]:\n",
        "                        review_field_name = field\n",
        "                        print(f\"Found review text in field: '{field}'\")\n",
        "                        break\n",
        "\n",
        "                # Find review ID field\n",
        "                for field in potential_id_fields:\n",
        "                    if field in reviews[0]:\n",
        "                        review_id_field = field\n",
        "                        print(f\"Found review ID in field: '{field}'\")\n",
        "                        break\n",
        "\n",
        "                if not review_field_name:\n",
        "                    print(\"Could not identify review text field. Available fields:\")\n",
        "                    print(list(reviews[0].keys()) if reviews else \"No reviews found\")\n",
        "                    return []\n",
        "\n",
        "                # Process each review\n",
        "                for i, review in enumerate(reviews):\n",
        "                    if i >= 1000:  # Limit for efficiency\n",
        "                        break\n",
        "\n",
        "                    total_reviews += 1\n",
        "                    text = review.get(review_field_name, '')\n",
        "\n",
        "                    if not text or not isinstance(text, str):\n",
        "                        continue\n",
        "\n",
        "                    # Preprocess the text\n",
        "                    processed_tokens = preprocess_text(text)\n",
        "\n",
        "                    if processed_tokens:  # Only add non-empty documents\n",
        "                        # Use actual review ID if available, otherwise generate one\n",
        "                        if review_id_field and review.get(review_id_field):\n",
        "                            review_id = str(review.get(review_id_field))\n",
        "                        else:\n",
        "                            review_id = f\"review_{i}\"\n",
        "\n",
        "                        documents.append({\n",
        "                            'review_id': review_id,\n",
        "                            'text': text,\n",
        "                            'processed_text': ' '.join(processed_tokens),  # For TF-IDF\n",
        "                            'tokens': processed_tokens,\n",
        "                            'stars': review.get('stars', 0),\n",
        "                            'business_id': review.get('business_id', '')\n",
        "                        })\n",
        "\n",
        "            except json.JSONDecodeError:\n",
        "                # Fallback to line-by-line parsing (JSONL format)\n",
        "                print(\"Single JSON parsing failed, trying line-by-line parsing...\")\n",
        "                f.seek(0)\n",
        "                for line_num, line in enumerate(f):\n",
        "                    if line_num >= 1000:  # Limit for efficiency\n",
        "                        break\n",
        "                    try:\n",
        "                        line = line.strip()\n",
        "                        if not line:\n",
        "                            continue\n",
        "                        review = json.loads(line)\n",
        "\n",
        "                        # Extract text\n",
        "                        text = review.get('text', '')\n",
        "                        if not text:\n",
        "                            continue\n",
        "\n",
        "                        # Preprocess the text\n",
        "                        processed_tokens = preprocess_text(text)\n",
        "\n",
        "                        if processed_tokens:\n",
        "                            documents.append({\n",
        "                                'review_id': review.get('review_id', f'review_{line_num}'),\n",
        "                                'text': text,\n",
        "                                'processed_text': ' '.join(processed_tokens),\n",
        "                                'tokens': processed_tokens,\n",
        "                                'stars': review.get('stars', 0),\n",
        "                                'business_id': review.get('business_id', '')\n",
        "                            })\n",
        "                        total_reviews += 1\n",
        "\n",
        "                    except json.JSONDecodeError:\n",
        "                        continue\n",
        "                    except Exception as e:\n",
        "                        print(f\"Skipping line {line_num}: {e}\")\n",
        "                        continue\n",
        "\n",
        "        print(f\"✓ Processed {total_reviews} reviews\")\n",
        "        print(f\"✓ Loaded {len(documents)} valid documents with preprocessing\")\n",
        "        return documents\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"✗ File not found: {file_path}\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Error loading data: {e}\")\n",
        "        return []\n",
        "\n",
        "class LLMRetriever:\n",
        "    \"\"\"LLM-based document retrieval system using OpenRouter\"\"\"\n",
        "\n",
        "    def __init__(self, api_key: str = None, model_name: str = CHOSEN_LLM_MODEL):\n",
        "        self.client = client\n",
        "        self.model_name = model_name\n",
        "        self.request_delay = 1  # Delay between API calls to avoid rate limits\n",
        "\n",
        "    def get_candidates_tfidf(self, query: str, documents: List[Dict], top_k: int = 5) -> Tuple[List[str], List[str]]:\n",
        "        \"\"\"Get candidate documents using TF-IDF for initial filtering with preprocessing\"\"\"\n",
        "        print(f\"Getting TF-IDF candidates for query: '{query}'\")\n",
        "\n",
        "        # Preprocess the query\n",
        "        query_tokens = preprocess_text(query)\n",
        "        processed_query = ' '.join(query_tokens)\n",
        "        print(f\"Processed query: '{processed_query}'\")\n",
        "\n",
        "        # Extract preprocessed texts\n",
        "        processed_texts = [doc['processed_text'] for doc in documents]\n",
        "        doc_ids = [doc['review_id'] for doc in documents]\n",
        "\n",
        "        # Create TF-IDF vectors using preprocessed text\n",
        "        vectorizer = TfidfVectorizer(\n",
        "            stop_words=None,  # We already removed stopwords during preprocessing\n",
        "            max_features=1000,\n",
        "            ngram_range=(1, 2),\n",
        "            min_df=2,\n",
        "            lowercase=False  # Already lowercased during preprocessing\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            doc_vectors = vectorizer.fit_transform(processed_texts)\n",
        "            query_vector = vectorizer.transform([processed_query])\n",
        "\n",
        "            # Calculate similarities\n",
        "            similarities = cosine_similarity(query_vector, doc_vectors).flatten()\n",
        "            top_indices = np.argsort(similarities)[::-1][:top_k]\n",
        "\n",
        "            candidate_docs = [documents[i]['text'] for i in top_indices]  # Use original text for LLM\n",
        "            candidate_ids = [doc_ids[i] for i in top_indices]\n",
        "\n",
        "            print(f\"Selected {len(candidate_docs)} candidates\")\n",
        "            top_sims = [f\"{similarities[i]:.4f}\" for i in top_indices[:3]]\n",
        "            print(f\"Top similarities: {top_sims}\")\n",
        "            return candidate_ids, candidate_docs\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"TF-IDF error: {e}, falling back to first {top_k} documents\")\n",
        "            return doc_ids[:top_k], [doc['text'] for doc in documents[:top_k]]\n",
        "\n",
        "    def llm_rerank(self, query: str, candidate_docs: List[str], candidate_ids: List[str],\n",
        "                   prompt_strategy: str = \"zero_shot\") -> List[str]:\n",
        "        \"\"\"Use LLM to re-rank candidate documents\"\"\"\n",
        "\n",
        "        print(f\"Re-ranking with LLM using '{prompt_strategy}' strategy\")\n",
        "\n",
        "        # Build prompt based on strategy\n",
        "        if prompt_strategy == \"zero_shot\":\n",
        "            prompt = self._build_zero_shot_prompt(query, candidate_docs, candidate_ids)\n",
        "        elif prompt_strategy == \"instruction_based\":\n",
        "            prompt = self._build_instruction_prompt(query, candidate_docs, candidate_ids)\n",
        "        elif prompt_strategy == \"detailed_analysis\":\n",
        "            prompt = self._build_detailed_prompt(query, candidate_docs, candidate_ids)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown prompt strategy: {prompt_strategy}\")\n",
        "\n",
        "        try:\n",
        "            # Add delay to avoid rate limits\n",
        "            time.sleep(self.request_delay)\n",
        "\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=self.model_name,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                max_tokens=300,\n",
        "                temperature=0.1  # Low temperature for consistency\n",
        "            )\n",
        "\n",
        "            llm_response = response.choices[0].message.content\n",
        "            print(f\"LLM Response: {llm_response[:100]}...\")\n",
        "\n",
        "            # Parse the response to extract selected document IDs\n",
        "            selected_ids = self._parse_llm_response(llm_response, candidate_ids)\n",
        "            return selected_ids\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"LLM API Error: {e}\")\n",
        "            # Fallback to TF-IDF ranking\n",
        "            return candidate_ids[:3]\n",
        "\n",
        "    def _build_zero_shot_prompt(self, query: str, docs: List[str], ids: List[str]) -> str:\n",
        "        \"\"\"Build zero-shot re-ranking prompt\"\"\"\n",
        "        doc_text = \"\"\n",
        "        for i, (doc, doc_id) in enumerate(zip(docs, ids)):\n",
        "            # Truncate long reviews to fit context window\n",
        "            truncated_doc = doc[:300] + \"...\" if len(doc) > 300 else doc\n",
        "            doc_text += f\"Review {i+1} (ID: {doc_id}): {truncated_doc}\\n\\n\"\n",
        "\n",
        "        return f\"\"\"Given the query '{query}' and the following restaurant reviews, which 3 reviews are most relevant to the query?\n",
        "\n",
        "Provide only the review numbers (1, 2, 3, etc.) in order of relevance, separated by commas.\n",
        "\n",
        "{doc_text}\n",
        "\n",
        "Most relevant reviews (numbers only):\"\"\"\n",
        "\n",
        "    def _build_instruction_prompt(self, query: str, docs: List[str], ids: List[str]) -> str:\n",
        "        \"\"\"Build instruction-based prompt\"\"\"\n",
        "        doc_text = \"\"\n",
        "        for i, (doc, doc_id) in enumerate(zip(docs, ids)):\n",
        "            truncated_doc = doc[:300] + \"...\" if len(doc) > 300 else doc\n",
        "            doc_text += f\"Review {i+1} (ID: {doc_id}): {truncated_doc}\\n\\n\"\n",
        "\n",
        "        return f\"\"\"You are a helpful assistant specializing in restaurant review analysis.\n",
        "I am searching for restaurant reviews about: '{query}'\n",
        "\n",
        "From the reviews below, identify the top 3 reviews that best match this search query.\n",
        "Focus on reviews that specifically mention or relate to the query topic.\n",
        "Consider both direct mentions and contextual relevance.\n",
        "\n",
        "Return only the review numbers (1, 2, 3, etc.) in order of relevance, separated by commas.\n",
        "\n",
        "Reviews:\n",
        "{doc_text}\n",
        "\n",
        "Top 3 most relevant reviews (numbers only):\"\"\"\n",
        "\n",
        "    def _build_detailed_prompt(self, query: str, docs: List[str], ids: List[str]) -> str:\n",
        "        \"\"\"Build detailed analysis prompt\"\"\"\n",
        "        doc_text = \"\"\n",
        "        for i, (doc, doc_id) in enumerate(zip(docs, ids)):\n",
        "            truncated_doc = doc[:300] + \"...\" if len(doc) > 300 else doc\n",
        "            doc_text += f\"Review {i+1} (ID: {doc_id}): {truncated_doc}\\n\\n\"\n",
        "\n",
        "        return f\"\"\"Analyze the following restaurant reviews to find the 3 most relevant ones for the query: '{query}'\n",
        "\n",
        "Consider these factors when ranking:\n",
        "1. Direct mentions of the query topic\n",
        "2. Contextual relevance and semantic similarity\n",
        "3. Quality and detail of information related to the query\n",
        "4. Overall helpfulness for someone searching for '{query}'\n",
        "\n",
        "Reviews:\n",
        "{doc_text}\n",
        "\n",
        "Based on your analysis, provide the top 3 review numbers in order of relevance (numbers only, separated by commas):\"\"\"\n",
        "\n",
        "    def _parse_llm_response(self, response: str, candidate_ids: List[str]) -> List[str]:\n",
        "        \"\"\"Parse LLM response to extract document IDs\"\"\"\n",
        "        # Look for numbers in the response\n",
        "        numbers = re.findall(r'\\b([1-9])\\b', response)\n",
        "\n",
        "        selected_ids = []\n",
        "        for num_str in numbers[:3]:  # Take only first 3\n",
        "            try:\n",
        "                idx = int(num_str) - 1  # Convert to 0-based index\n",
        "                if 0 <= idx < len(candidate_ids):\n",
        "                    selected_ids.append(candidate_ids[idx])\n",
        "            except ValueError:\n",
        "                continue\n",
        "\n",
        "        # If we don't have enough, pad with remaining candidates\n",
        "        while len(selected_ids) < 3 and len(selected_ids) < len(candidate_ids):\n",
        "            for cid in candidate_ids:\n",
        "                if cid not in selected_ids:\n",
        "                    selected_ids.append(cid)\n",
        "                    break\n",
        "\n",
        "        return selected_ids[:3]\n",
        "\n",
        "def run_llm_retrieval_experiment():\n",
        "    \"\"\"Main function to run the LLM retrieval experiment\"\"\"\n",
        "\n",
        "    # Test API connection first\n",
        "    if not test_api_connection():\n",
        "        print(\"Cannot proceed without API connection. Please check your API key.\")\n",
        "        return\n",
        "\n",
        "    # Load Yelp data - now handles directory paths and includes preprocessing\n",
        "    reviews_data = load_yelp_data(\"/content/sample_data/yelp\")\n",
        "\n",
        "    if not reviews_data:\n",
        "        print(\"No data loaded. Cannot proceed with experiment.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Loaded {len(reviews_data)} preprocessed documents\")\n",
        "\n",
        "    # Define test queries (use the same 10 queries from previous tasks)\n",
        "    test_queries = [\n",
        "        \"general chicken\",\n",
        "        \"fried chicken\",\n",
        "        \"BBQ sandwiches\",\n",
        "        \"mashed potatoes\",\n",
        "        \"Grilled Shrimp Salad\",\n",
        "        \"lamb Shank\",\n",
        "        \"Pepperoni pizza\",\n",
        "        \"brussel sprout salad\",\n",
        "        \"FRIENDLY STAFF\",\n",
        "        \"Grilled Cheese\"\n",
        "    ]\n",
        "\n",
        "    # Initialize retriever\n",
        "    retriever = LLMRetriever()\n",
        "\n",
        "    # Test different prompt strategies\n",
        "    prompt_strategies = [\"zero_shot\", \"instruction_based\", \"detailed_analysis\"]\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    print(f\"\\n=== Running LLM Retrieval Experiment ===\")\n",
        "    print(f\"Testing {len(test_queries)} queries with {len(prompt_strategies)} prompt strategies\")\n",
        "\n",
        "    for query in test_queries:\n",
        "        print(f\"\\n--- Processing Query: '{query}' ---\")\n",
        "        results[query] = {}\n",
        "\n",
        "        # Get TF-IDF candidates first (now uses preprocessed documents)\n",
        "        candidate_ids, candidate_docs = retriever.get_candidates_tfidf(\n",
        "            query, reviews_data, top_k=5\n",
        "        )\n",
        "\n",
        "        # Test each prompt strategy\n",
        "        for strategy in prompt_strategies:\n",
        "            print(f\"\\nTesting strategy: {strategy}\")\n",
        "            try:\n",
        "                selected_ids = retriever.llm_rerank(\n",
        "                    query, candidate_docs, candidate_ids, strategy\n",
        "                )\n",
        "                results[query][strategy] = selected_ids\n",
        "                print(f\"Selected documents: {selected_ids}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error with strategy {strategy}: {e}\")\n",
        "                results[query][strategy] = candidate_ids[:3]  # Fallback\n",
        "\n",
        "    return results, reviews_data\n",
        "\n",
        "def analyze_and_report_results(results: Dict, reviews_data: List[Dict]):\n",
        "    \"\"\"Analyze and report the LLM retrieval results with document details\"\"\"\n",
        "    print(f\"\\n=== LLM Retrieval Results Analysis ===\")\n",
        "\n",
        "    # Report results for each query with document previews\n",
        "    for query, strategies in results.items():\n",
        "        print(f\"\\nQuery: '{query}'\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        # Find the documents for better analysis\n",
        "        doc_lookup = {doc['review_id']: doc for doc in reviews_data}\n",
        "\n",
        "        for strategy, doc_ids in strategies.items():\n",
        "            print(f\"\\n{strategy.upper()} Results:\")\n",
        "            for i, doc_id in enumerate(doc_ids, 1):\n",
        "                if doc_id in doc_lookup:\n",
        "                    doc = doc_lookup[doc_id]\n",
        "                    text_preview = doc['text'][:150] + \"...\" if len(doc['text']) > 150 else doc['text']\n",
        "                    print(f\"  {i}. ID: {doc_id}\")\n",
        "                    print(f\"     Preview: {text_preview}\")\n",
        "                    print(f\"     Stars: {doc.get('stars', 'N/A')}\")\n",
        "                else:\n",
        "                    print(f\"  {i}. ID: {doc_id} (document not found)\")\n",
        "\n",
        "    # Calculate strategy consistency\n",
        "    print(f\"\\n=== Strategy Consistency Analysis ===\")\n",
        "    for query in results:\n",
        "        strategies = list(results[query].keys())\n",
        "        if len(strategies) >= 2:\n",
        "            overlap_counts = {}\n",
        "            for i, strategy1 in enumerate(strategies):\n",
        "                for strategy2 in strategies[i+1:]:\n",
        "                    docs1 = set(results[query][strategy1])\n",
        "                    docs2 = set(results[query][strategy2])\n",
        "                    overlap = len(docs1.intersection(docs2))\n",
        "                    pair = f\"{strategy1} vs {strategy2}\"\n",
        "                    if pair not in overlap_counts:\n",
        "                        overlap_counts[pair] = []\n",
        "                    overlap_counts[pair].append(overlap)\n",
        "\n",
        "            print(f\"\\nQuery '{query}' - Strategy overlaps:\")\n",
        "            for pair, overlaps in overlap_counts.items():\n",
        "                avg_overlap = sum(overlaps) / len(overlaps)\n",
        "                print(f\"  {pair}: {avg_overlap:.1f}/3 documents overlap\")\n",
        "\n",
        "# Part 1: Conceptual explanation (already covered in the guide)\n",
        "print(\"\\n=== Part 1: Introduction to LLM-based Retrieval ===\")\n",
        "print(\"\"\"\n",
        "LLM-based Retrieval leverages large language models' natural language understanding\n",
        "to identify relevant documents based on semantic meaning rather than just keyword matching.\n",
        "\n",
        "Advantages over traditional methods:\n",
        "- Better semantic understanding and context awareness\n",
        "- Can handle complex, conversational queries\n",
        "- Understands synonyms, paraphrases, and implicit meanings\n",
        "\n",
        "Disadvantages:\n",
        "- Higher computational cost and slower processing\n",
        "- Limited by context window size\n",
        "- Less consistent due to probabilistic nature\n",
        "- Risk of hallucination in relevance judgments\n",
        "\n",
        "\"\"\")\n",
        "\n",
        "# Part 2: Run the experiment\n",
        "if __name__ == \"__main__\":\n",
        "    # Run the main experiment\n",
        "    experiment_results, reviews_data = run_llm_retrieval_experiment()\n",
        "\n",
        "    if experiment_results:\n",
        "        # Analyze and report results\n",
        "        analyze_and_report_results(experiment_results, reviews_data)\n",
        "\n",
        "        # Save results to file with additional metadata\n",
        "        output_data = {\n",
        "            'experiment_results': experiment_results,\n",
        "            'metadata': {\n",
        "                'total_documents': len(reviews_data),\n",
        "                'queries_tested': len(experiment_results),\n",
        "                'strategies_used': ['zero_shot', 'instruction_based', 'detailed_analysis'],\n",
        "                'preprocessing_applied': True,\n",
        "                'model_used': CHOSEN_LLM_MODEL\n",
        "            }\n",
        "        }\n",
        "\n",
        "        with open('llm_retrieval_results.json', 'w') as f:\n",
        "            json.dump(output_data, f, indent=2)\n",
        "        print(f\"\\nResults saved to 'llm_retrieval_results.json'\")\n",
        "\n",
        "        print(f\"\\n=== Experiment Complete ===\")\n",
        "        print(f\"✓ Tested {len(experiment_results)} queries\")\n",
        "        print(f\"✓ Used 3 different prompt strategies\")\n",
        "        print(f\"✓ Applied consistent preprocessing from previous tasks\")\n",
        "        print(f\"✓ Implemented re-ranking approach with TF-IDF candidates\")\n",
        "    else:\n",
        "        print(\"Experiment failed. Please check your setup and try again.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUoU9aG40MbT",
        "outputId": "b5a383de-c5b5-4f71-f72f-f086e6b1df0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task 3: Retrieval with LLMs via OpenRouter\n",
            "\n",
            "=== Part 1: Introduction to LLM-based Retrieval ===\n",
            "\n",
            "LLM-based Retrieval leverages large language models' natural language understanding \n",
            "to identify relevant documents based on semantic meaning rather than just keyword matching.\n",
            "\n",
            "Advantages over traditional methods:\n",
            "- Better semantic understanding and context awareness\n",
            "- Can handle complex, conversational queries\n",
            "- Understands synonyms, paraphrases, and implicit meanings\n",
            "\n",
            "Disadvantages:\n",
            "- Higher computational cost and slower processing\n",
            "- Limited by context window size\n",
            "- Less consistent due to probabilistic nature\n",
            "- Risk of hallucination in relevance judgments\n",
            "\n",
            "Chosen model: mistralai/mistral-7b-instruct:free\n",
            "\n",
            "\n",
            "=== Testing API Connection ===\n",
            "✓ API Connection Successful!\n",
            "Test Response: \n",
            "\n",
            "=== Loading Yelp Data from /content/sample_data/yelp ===\n",
            "Path is a directory. Searching for review files...\n",
            "\n",
            "=== Scanning directory: /content/sample_data/yelp ===\n",
            "Found files: ['/content/sample_data/yelp/dlDEuDIvZI6I0cGZy4jIYg.json', '/content/sample_data/yelp/okChDSotPCRtJlQVzlnw1Q.json', '/content/sample_data/yelp/pVEyB1BxiZkJUeoqHG3ehA.json', '/content/sample_data/yelp/d5YWKrP-zG74nqOYzHn7Zg.json', '/content/sample_data/yelp/ExDgGmuZJOsGXAzSIM1jtA.json', '/content/sample_data/yelp/7lnOgOMd7zys3bFfWY_jIw.json', '/content/sample_data/yelp/NQqSTW5c6bnsfVZaZAON1g.json', '/content/sample_data/yelp/Z04-eU8EKLvnEsdX9ngqVQ.json', '/content/sample_data/yelp/gB-0XpxMvsO04AMedpQs1Q.json', '/content/sample_data/yelp/tEn9OKYKMVtVWiBP3NmcEg.json', '/content/sample_data/yelp/zkjcbVWPZHRzYdVzTIOA_Q.json', '/content/sample_data/yelp/sWXT0sNodEnxZjziB3saZQ.json', '/content/sample_data/yelp/8d-RAa59tgV06VRa5SOe9A.json', '/content/sample_data/yelp/J_rNhtb144k_fW50UQ7_lg.json', '/content/sample_data/yelp/CvmAN25laBbwZTqmlEAR1Q.json', '/content/sample_data/yelp/0qnMSq3gYhGDesZKxNgAkQ.json', '/content/sample_data/yelp/7pezxyAK-7l5Yfr7W7fFQQ.json', '/content/sample_data/yelp/aq_UTxlYvMNEN-urxbUddA.json', '/content/sample_data/yelp/dCw8uBtwnJFKwMnASlZLzg.json', '/content/sample_data/yelp/A9xPHLcWtRgK6Mf4-ksBrw.json', '/content/sample_data/yelp/y5EMn3YMd9tmUJ-guKS4eQ.json', '/content/sample_data/yelp/3OLZOlqgOXdqY0uwxcOTfw.json', '/content/sample_data/yelp/9N0YqwhE1qwU4OmhzwGjtA.json', '/content/sample_data/yelp/Ud3xHfA2EP5BsmugEHwrhQ.json', '/content/sample_data/yelp/F_YQ3BrfHSMmKvJ_nqyYAA.json', '/content/sample_data/yelp/b7jDtbG7UloYasZ9sVXHWg.json', '/content/sample_data/yelp/lAPSCXomy_O3etWLxuQNUQ.json', '/content/sample_data/yelp/e1grgYnCxbue-tU5Gf6IlQ.json', '/content/sample_data/yelp/VAi6XF1fbsx6LOjpLkPuTg.json', '/content/sample_data/yelp/UKtzNIgmUd-9TljoI2wLiA.json', '/content/sample_data/yelp/tNnNr1AJf6ThLsSnPdd3xA.json', '/content/sample_data/yelp/SmYVU8I4ukBd1zJWdIxPcA.json', '/content/sample_data/yelp/rzHGofdCbaskvo2SzC6Vpg.json', '/content/sample_data/yelp/p8Uu-CYOUaISZT4w6OTNrQ.json', '/content/sample_data/yelp/l2rPzn4wx2UhRaF5LKVJwA.json', '/content/sample_data/yelp/t9wUtmbqYZ2jZf5GcbOKnQ.json', '/content/sample_data/yelp/9IRdWhDNo2T6vyMLwrQdMw.json', '/content/sample_data/yelp/s46fIpN6IbmMxKKkyNGBGw.json', '/content/sample_data/yelp/Is7TtkCabINOEq7e-TiWXA.json', '/content/sample_data/yelp/AZrLjmV0A2TzOCkZ6CU-Fg.json', '/content/sample_data/yelp/6CvMYmV0Uw4Y4G7NpvTgfg.json', '/content/sample_data/yelp/gQCn4Gv-4_UUUFwpo-zHvA.json', '/content/sample_data/yelp/z90atI98B7g1UvWn_i9ViQ.json', '/content/sample_data/yelp/SlBFeVyO-ewdi6cC-fVVPA.json', '/content/sample_data/yelp/kO4SS2cYzFyUM04EUSGMpg.json', '/content/sample_data/yelp/8y00IHK9sbA1Zhl2E9hmpA.json', '/content/sample_data/yelp/tDpy78JYTqrClowdaJc6kA.json', '/content/sample_data/yelp/9GVazUhx0dYB-_H0hlgB5w.json', '/content/sample_data/yelp/Q6nXAEw3UuAVFSztE4lPnA.json', '/content/sample_data/yelp/EsypDkgdOEurddgPFyhCqw.json', '/content/sample_data/yelp/Nl8kv24kLtlGn5aqoggu8A.json', '/content/sample_data/yelp/vTKSLe9DwRelou9wtcmHUA.json', '/content/sample_data/yelp/4lgk5AJvmoXPrfSlCyjiQg.json', '/content/sample_data/yelp/UTMAQluppsu2ZDDaKKZyIQ.json', '/content/sample_data/yelp/XJf0g9xKrMM-8uu1Mj4qsA.json', '/content/sample_data/yelp/z5HiFhPj93NYrsrHJ86Wnw.json', '/content/sample_data/yelp/3VCZ21-DIw7voVexDMXDSA.json', '/content/sample_data/yelp/zdzVyuYhpF5R2oiLuL0Rrw.json', '/content/sample_data/yelp/FtRrpU4mSRAkLNmLMGQ_Dg.json', '/content/sample_data/yelp/WU_dFObt9VxHMcS1Eu32iQ.json']\n",
            "Using file: /content/sample_data/yelp/dlDEuDIvZI6I0cGZy4jIYg.json\n",
            "Found 1045 reviews in a single JSON object\n",
            "Found review text in field: 'Content'\n",
            "Found review ID in field: 'ReviewID'\n",
            "✓ Processed 1000 reviews\n",
            "✓ Loaded 1000 valid documents with preprocessing\n",
            "Loaded 1000 preprocessed documents\n",
            "\n",
            "=== Running LLM Retrieval Experiment ===\n",
            "Testing 10 queries with 3 prompt strategies\n",
            "\n",
            "--- Processing Query: 'general chicken' ---\n",
            "Getting TF-IDF candidates for query: 'general chicken'\n",
            "Processed query: 'gener chicken'\n",
            "Selected 5 candidates\n",
            "Top similarities: ['0.2039', '0.1987', '0.1905']\n",
            "\n",
            "Testing strategy: zero_shot\n",
            "Re-ranking with LLM using 'zero_shot' strategy\n",
            "LLM Response: ...\n",
            "Selected documents: ['bI597D-A9A7BiF8TpqiK6Q', '9JdMXFUPVYlokLhwoqxCXA', '47Zw_31GCB8IazoHkmkaEw']\n",
            "\n",
            "Testing strategy: instruction_based\n",
            "Re-ranking with LLM using 'instruction_based' strategy\n",
            "LLM Response: ...\n",
            "Selected documents: ['bI597D-A9A7BiF8TpqiK6Q', '9JdMXFUPVYlokLhwoqxCXA', '47Zw_31GCB8IazoHkmkaEw']\n",
            "\n",
            "Testing strategy: detailed_analysis\n",
            "Re-ranking with LLM using 'detailed_analysis' strategy\n",
            "LLM Response: ...\n",
            "Selected documents: ['bI597D-A9A7BiF8TpqiK6Q', '9JdMXFUPVYlokLhwoqxCXA', '47Zw_31GCB8IazoHkmkaEw']\n",
            "\n",
            "--- Processing Query: 'fried chicken' ---\n",
            "Getting TF-IDF candidates for query: 'fried chicken'\n",
            "Processed query: 'fri chicken'\n",
            "Selected 5 candidates\n",
            "Top similarities: ['0.2429', '0.2411', '0.2350']\n",
            "\n",
            "Testing strategy: zero_shot\n",
            "Re-ranking with LLM using 'zero_shot' strategy\n",
            "LLM Response: ...\n",
            "Selected documents: ['FhsIAy1sLba19YCC7mhCgw', 'bI597D-A9A7BiF8TpqiK6Q', '9JdMXFUPVYlokLhwoqxCXA']\n",
            "\n",
            "Testing strategy: instruction_based\n",
            "Re-ranking with LLM using 'instruction_based' strategy\n",
            "LLM Response: ...\n",
            "Selected documents: ['FhsIAy1sLba19YCC7mhCgw', 'bI597D-A9A7BiF8TpqiK6Q', '9JdMXFUPVYlokLhwoqxCXA']\n",
            "\n",
            "Testing strategy: detailed_analysis\n",
            "Re-ranking with LLM using 'detailed_analysis' strategy\n",
            "LLM Response: ...\n",
            "Selected documents: ['FhsIAy1sLba19YCC7mhCgw', 'bI597D-A9A7BiF8TpqiK6Q', '9JdMXFUPVYlokLhwoqxCXA']\n",
            "\n",
            "--- Processing Query: 'BBQ sandwiches' ---\n",
            "Getting TF-IDF candidates for query: 'BBQ sandwiches'\n",
            "Processed query: 'bbq sandwich'\n",
            "Selected 5 candidates\n",
            "Top similarities: ['0.0000', '0.0000', '0.0000']\n",
            "\n",
            "Testing strategy: zero_shot\n",
            "Re-ranking with LLM using 'zero_shot' strategy\n",
            "LLM Response: ...\n",
            "Selected documents: ['WUiaWPD7C9fmdAfhmk-QZw', 'U6ieGW7vgpiPZMZODrQRaw', 'Zp9iVKa-6EgxREBtDhDkBg']\n",
            "\n",
            "Testing strategy: instruction_based\n",
            "Re-ranking with LLM using 'instruction_based' strategy\n",
            "LLM Response: ...\n",
            "Selected documents: ['WUiaWPD7C9fmdAfhmk-QZw', 'U6ieGW7vgpiPZMZODrQRaw', 'Zp9iVKa-6EgxREBtDhDkBg']\n",
            "\n",
            "Testing strategy: detailed_analysis\n",
            "Re-ranking with LLM using 'detailed_analysis' strategy\n",
            "LLM Response: ...\n",
            "Selected documents: ['WUiaWPD7C9fmdAfhmk-QZw', 'U6ieGW7vgpiPZMZODrQRaw', 'Zp9iVKa-6EgxREBtDhDkBg']\n",
            "\n",
            "--- Processing Query: 'mashed potatoes' ---\n",
            "Getting TF-IDF candidates for query: 'mashed potatoes'\n",
            "Processed query: 'mash potato'\n",
            "Selected 5 candidates\n",
            "Top similarities: ['0.2718', '0.2708', '0.2680']\n",
            "\n",
            "Testing strategy: zero_shot\n",
            "Re-ranking with LLM using 'zero_shot' strategy\n",
            "LLM Response: ...\n",
            "Selected documents: ['erhoC7CP8KTsURyo0Dq6eQ', 'U_JreUiWJpqEtpTao8mqBA', 'ylkgbFWvvxXIrp_NMlOW3g']\n",
            "\n",
            "Testing strategy: instruction_based\n",
            "Re-ranking with LLM using 'instruction_based' strategy\n",
            "LLM Response: ...\n",
            "Selected documents: ['erhoC7CP8KTsURyo0Dq6eQ', 'U_JreUiWJpqEtpTao8mqBA', 'ylkgbFWvvxXIrp_NMlOW3g']\n",
            "\n",
            "Testing strategy: detailed_analysis\n",
            "Re-ranking with LLM using 'detailed_analysis' strategy\n",
            "LLM Response: ...\n",
            "Selected documents: ['erhoC7CP8KTsURyo0Dq6eQ', 'U_JreUiWJpqEtpTao8mqBA', 'ylkgbFWvvxXIrp_NMlOW3g']\n",
            "\n",
            "--- Processing Query: 'Grilled Shrimp Salad' ---\n",
            "Getting TF-IDF candidates for query: 'Grilled Shrimp Salad'\n",
            "Processed query: 'grill shrimp salad'\n",
            "Selected 5 candidates\n",
            "Top similarities: ['0.2971', '0.2366', '0.2153']\n",
            "\n",
            "Testing strategy: zero_shot\n",
            "Re-ranking with LLM using 'zero_shot' strategy\n",
            "LLM Response: ...\n",
            "Selected documents: ['HuACkycdLUNsquqfrSwjNg', 'mGwM8Djg6KmiZeKRiGnvnQ', 'q1iJuca2tXcGl22RAv42KA']\n",
            "\n",
            "Testing strategy: instruction_based\n",
            "Re-ranking with LLM using 'instruction_based' strategy\n",
            "LLM Response: ...\n",
            "Selected documents: ['HuACkycdLUNsquqfrSwjNg', 'mGwM8Djg6KmiZeKRiGnvnQ', 'q1iJuca2tXcGl22RAv42KA']\n",
            "\n",
            "Testing strategy: detailed_analysis\n",
            "Re-ranking with LLM using 'detailed_analysis' strategy\n",
            "LLM Response: ...\n",
            "Selected documents: ['HuACkycdLUNsquqfrSwjNg', 'mGwM8Djg6KmiZeKRiGnvnQ', 'q1iJuca2tXcGl22RAv42KA']\n",
            "\n",
            "--- Processing Query: 'lamb Shank' ---\n",
            "Getting TF-IDF candidates for query: 'lamb Shank'\n",
            "Processed query: 'lamb shank'\n",
            "Selected 5 candidates\n",
            "Top similarities: ['0.0000', '0.0000', '0.0000']\n",
            "\n",
            "Testing strategy: zero_shot\n",
            "Re-ranking with LLM using 'zero_shot' strategy\n",
            "LLM Response: ...\n",
            "Selected documents: ['WUiaWPD7C9fmdAfhmk-QZw', 'U6ieGW7vgpiPZMZODrQRaw', 'Zp9iVKa-6EgxREBtDhDkBg']\n",
            "\n",
            "Testing strategy: instruction_based\n",
            "Re-ranking with LLM using 'instruction_based' strategy\n",
            "LLM Response: ...\n",
            "Selected documents: ['WUiaWPD7C9fmdAfhmk-QZw', 'U6ieGW7vgpiPZMZODrQRaw', 'Zp9iVKa-6EgxREBtDhDkBg']\n",
            "\n",
            "Testing strategy: detailed_analysis\n",
            "Re-ranking with LLM using 'detailed_analysis' strategy\n",
            "LLM Response: ...\n",
            "Selected documents: ['WUiaWPD7C9fmdAfhmk-QZw', 'U6ieGW7vgpiPZMZODrQRaw', 'Zp9iVKa-6EgxREBtDhDkBg']\n",
            "\n",
            "--- Processing Query: 'Pepperoni pizza' ---\n",
            "Getting TF-IDF candidates for query: 'Pepperoni pizza'\n",
            "Processed query: 'pepperoni pizza'\n",
            "Selected 5 candidates\n",
            "Top similarities: ['0.3059', '0.2698', '0.2677']\n",
            "\n",
            "Testing strategy: zero_shot\n",
            "Re-ranking with LLM using 'zero_shot' strategy\n",
            "LLM Response: ...\n",
            "Selected documents: ['AvxVlBdhKtV1ZaBFXTYChA', 'A6Hadnz8TaJrPLJ70N4R2g', 'RQhm_AIxYV-lQd3famtgsQ']\n",
            "\n",
            "Testing strategy: instruction_based\n",
            "Re-ranking with LLM using 'instruction_based' strategy\n",
            "LLM Response: ...\n",
            "Selected documents: ['AvxVlBdhKtV1ZaBFXTYChA', 'A6Hadnz8TaJrPLJ70N4R2g', 'RQhm_AIxYV-lQd3famtgsQ']\n",
            "\n",
            "Testing strategy: detailed_analysis\n",
            "Re-ranking with LLM using 'detailed_analysis' strategy\n",
            "LLM Response: ...\n",
            "Selected documents: ['AvxVlBdhKtV1ZaBFXTYChA', 'A6Hadnz8TaJrPLJ70N4R2g', 'RQhm_AIxYV-lQd3famtgsQ']\n",
            "\n",
            "--- Processing Query: 'brussel sprout salad' ---\n",
            "Getting TF-IDF candidates for query: 'brussel sprout salad'\n",
            "Processed query: 'brussel sprout salad'\n",
            "Selected 5 candidates\n",
            "Top similarities: ['0.5313', '0.5230', '0.4488']\n",
            "\n",
            "Testing strategy: zero_shot\n",
            "Re-ranking with LLM using 'zero_shot' strategy\n",
            "LLM Response: ...\n",
            "Selected documents: ['rsLQPvcBsJR1ztA3VpARPw', 'fHas60-vCPrtnk7SE5-2VQ', '1dGHjDcWJFQrl-1EyxujOQ']\n",
            "\n",
            "Testing strategy: instruction_based\n",
            "Re-ranking with LLM using 'instruction_based' strategy\n",
            "LLM Response: ...\n",
            "Selected documents: ['rsLQPvcBsJR1ztA3VpARPw', 'fHas60-vCPrtnk7SE5-2VQ', '1dGHjDcWJFQrl-1EyxujOQ']\n",
            "\n",
            "Testing strategy: detailed_analysis\n",
            "Re-ranking with LLM using 'detailed_analysis' strategy\n",
            "LLM Response: ...\n",
            "Selected documents: ['rsLQPvcBsJR1ztA3VpARPw', 'fHas60-vCPrtnk7SE5-2VQ', '1dGHjDcWJFQrl-1EyxujOQ']\n",
            "\n",
            "--- Processing Query: 'FRIENDLY STAFF' ---\n",
            "Getting TF-IDF candidates for query: 'FRIENDLY STAFF'\n",
            "Processed query: 'friendli staff'\n",
            "Selected 5 candidates\n",
            "Top similarities: ['0.3604', '0.3351', '0.3329']\n",
            "\n",
            "Testing strategy: zero_shot\n",
            "Re-ranking with LLM using 'zero_shot' strategy\n",
            "LLM Response: ...\n",
            "Selected documents: ['PV74AXAafSV3CYH8TF2BPQ', 'ho1wi0wccD4VNpifIt1Z4w', '1Lrt-DqBoncAyJFdYkJ2xw']\n",
            "\n",
            "Testing strategy: instruction_based\n",
            "Re-ranking with LLM using 'instruction_based' strategy\n",
            "LLM Response: ...\n",
            "Selected documents: ['PV74AXAafSV3CYH8TF2BPQ', 'ho1wi0wccD4VNpifIt1Z4w', '1Lrt-DqBoncAyJFdYkJ2xw']\n",
            "\n",
            "Testing strategy: detailed_analysis\n",
            "Re-ranking with LLM using 'detailed_analysis' strategy\n",
            "LLM Response: ...\n",
            "Selected documents: ['PV74AXAafSV3CYH8TF2BPQ', 'ho1wi0wccD4VNpifIt1Z4w', '1Lrt-DqBoncAyJFdYkJ2xw']\n",
            "\n",
            "--- Processing Query: 'Grilled Cheese' ---\n",
            "Getting TF-IDF candidates for query: 'Grilled Cheese'\n",
            "Processed query: 'grill chees'\n",
            "Selected 5 candidates\n",
            "Top similarities: ['0.2558', '0.2497', '0.2101']\n",
            "\n",
            "Testing strategy: zero_shot\n",
            "Re-ranking with LLM using 'zero_shot' strategy\n",
            "LLM Response: ...\n",
            "Selected documents: ['d7-dBa7uzGM3QxpJMXb1Yw', 'q1iJuca2tXcGl22RAv42KA', 'HuACkycdLUNsquqfrSwjNg']\n",
            "\n",
            "Testing strategy: instruction_based\n",
            "Re-ranking with LLM using 'instruction_based' strategy\n",
            "LLM Response: ...\n",
            "Selected documents: ['d7-dBa7uzGM3QxpJMXb1Yw', 'q1iJuca2tXcGl22RAv42KA', 'HuACkycdLUNsquqfrSwjNg']\n",
            "\n",
            "Testing strategy: detailed_analysis\n",
            "Re-ranking with LLM using 'detailed_analysis' strategy\n",
            "LLM Response: ...\n",
            "Selected documents: ['d7-dBa7uzGM3QxpJMXb1Yw', 'q1iJuca2tXcGl22RAv42KA', 'HuACkycdLUNsquqfrSwjNg']\n",
            "\n",
            "=== LLM Retrieval Results Analysis ===\n",
            "\n",
            "Query: 'general chicken'\n",
            "--------------------------------------------------\n",
            "\n",
            "ZERO_SHOT Results:\n",
            "  1. ID: bI597D-A9A7BiF8TpqiK6Q\n",
            "     Preview: This place is good but not as good as I was expecting. Pasta is good. Chicken was mediocre. Had a special flatbread they were offering with kale and i...\n",
            "     Stars: 0\n",
            "  2. ID: 9JdMXFUPVYlokLhwoqxCXA\n",
            "     Preview: This Place is awesome! The appetizers of bread that we ordered were homemade and the garlic and cheese that went with were delicious. I had the roaste...\n",
            "     Stars: 0\n",
            "  3. ID: 47Zw_31GCB8IazoHkmkaEw\n",
            "     Preview: Had the roasted half chicken and my boyfriend had the risotto. We both agreed that the food was very delicious but terribly overpriced. The ambiance/d...\n",
            "     Stars: 0\n",
            "\n",
            "INSTRUCTION_BASED Results:\n",
            "  1. ID: bI597D-A9A7BiF8TpqiK6Q\n",
            "     Preview: This place is good but not as good as I was expecting. Pasta is good. Chicken was mediocre. Had a special flatbread they were offering with kale and i...\n",
            "     Stars: 0\n",
            "  2. ID: 9JdMXFUPVYlokLhwoqxCXA\n",
            "     Preview: This Place is awesome! The appetizers of bread that we ordered were homemade and the garlic and cheese that went with were delicious. I had the roaste...\n",
            "     Stars: 0\n",
            "  3. ID: 47Zw_31GCB8IazoHkmkaEw\n",
            "     Preview: Had the roasted half chicken and my boyfriend had the risotto. We both agreed that the food was very delicious but terribly overpriced. The ambiance/d...\n",
            "     Stars: 0\n",
            "\n",
            "DETAILED_ANALYSIS Results:\n",
            "  1. ID: bI597D-A9A7BiF8TpqiK6Q\n",
            "     Preview: This place is good but not as good as I was expecting. Pasta is good. Chicken was mediocre. Had a special flatbread they were offering with kale and i...\n",
            "     Stars: 0\n",
            "  2. ID: 9JdMXFUPVYlokLhwoqxCXA\n",
            "     Preview: This Place is awesome! The appetizers of bread that we ordered were homemade and the garlic and cheese that went with were delicious. I had the roaste...\n",
            "     Stars: 0\n",
            "  3. ID: 47Zw_31GCB8IazoHkmkaEw\n",
            "     Preview: Had the roasted half chicken and my boyfriend had the risotto. We both agreed that the food was very delicious but terribly overpriced. The ambiance/d...\n",
            "     Stars: 0\n",
            "\n",
            "Query: 'fried chicken'\n",
            "--------------------------------------------------\n",
            "\n",
            "ZERO_SHOT Results:\n",
            "  1. ID: FhsIAy1sLba19YCC7mhCgw\n",
            "     Preview: RPM is simply awesome! We were a group of four and we shared 2 bottles of wine, the polenta, tuna tartare, Julianna's Salad, Caesar salad, fried artic...\n",
            "     Stars: 0\n",
            "  2. ID: bI597D-A9A7BiF8TpqiK6Q\n",
            "     Preview: This place is good but not as good as I was expecting. Pasta is good. Chicken was mediocre. Had a special flatbread they were offering with kale and i...\n",
            "     Stars: 0\n",
            "  3. ID: 9JdMXFUPVYlokLhwoqxCXA\n",
            "     Preview: This Place is awesome! The appetizers of bread that we ordered were homemade and the garlic and cheese that went with were delicious. I had the roaste...\n",
            "     Stars: 0\n",
            "\n",
            "INSTRUCTION_BASED Results:\n",
            "  1. ID: FhsIAy1sLba19YCC7mhCgw\n",
            "     Preview: RPM is simply awesome! We were a group of four and we shared 2 bottles of wine, the polenta, tuna tartare, Julianna's Salad, Caesar salad, fried artic...\n",
            "     Stars: 0\n",
            "  2. ID: bI597D-A9A7BiF8TpqiK6Q\n",
            "     Preview: This place is good but not as good as I was expecting. Pasta is good. Chicken was mediocre. Had a special flatbread they were offering with kale and i...\n",
            "     Stars: 0\n",
            "  3. ID: 9JdMXFUPVYlokLhwoqxCXA\n",
            "     Preview: This Place is awesome! The appetizers of bread that we ordered were homemade and the garlic and cheese that went with were delicious. I had the roaste...\n",
            "     Stars: 0\n",
            "\n",
            "DETAILED_ANALYSIS Results:\n",
            "  1. ID: FhsIAy1sLba19YCC7mhCgw\n",
            "     Preview: RPM is simply awesome! We were a group of four and we shared 2 bottles of wine, the polenta, tuna tartare, Julianna's Salad, Caesar salad, fried artic...\n",
            "     Stars: 0\n",
            "  2. ID: bI597D-A9A7BiF8TpqiK6Q\n",
            "     Preview: This place is good but not as good as I was expecting. Pasta is good. Chicken was mediocre. Had a special flatbread they were offering with kale and i...\n",
            "     Stars: 0\n",
            "  3. ID: 9JdMXFUPVYlokLhwoqxCXA\n",
            "     Preview: This Place is awesome! The appetizers of bread that we ordered were homemade and the garlic and cheese that went with were delicious. I had the roaste...\n",
            "     Stars: 0\n",
            "\n",
            "Query: 'BBQ sandwiches'\n",
            "--------------------------------------------------\n",
            "\n",
            "ZERO_SHOT Results:\n",
            "  1. ID: WUiaWPD7C9fmdAfhmk-QZw\n",
            "     Preview: RPM struggles in one sense. It can't decide if it's a small dish (ie tapas style) type of place or if it's just a normal high end restaurant. The plat...\n",
            "     Stars: 0\n",
            "  2. ID: U6ieGW7vgpiPZMZODrQRaw\n",
            "     Preview: I heard this place was OK and then ended up loving it! After seeing the show, I had to try Mama DePandi's Bucatini Pomodoro (even though I don't typic...\n",
            "     Stars: 0\n",
            "  3. ID: Zp9iVKa-6EgxREBtDhDkBg\n",
            "     Preview: Was here opening week. Loved the white jackets servers were wearing. The space itself was modern and clean. Not mind blowing, but elegant. Wine menu w...\n",
            "     Stars: 0\n",
            "\n",
            "INSTRUCTION_BASED Results:\n",
            "  1. ID: WUiaWPD7C9fmdAfhmk-QZw\n",
            "     Preview: RPM struggles in one sense. It can't decide if it's a small dish (ie tapas style) type of place or if it's just a normal high end restaurant. The plat...\n",
            "     Stars: 0\n",
            "  2. ID: U6ieGW7vgpiPZMZODrQRaw\n",
            "     Preview: I heard this place was OK and then ended up loving it! After seeing the show, I had to try Mama DePandi's Bucatini Pomodoro (even though I don't typic...\n",
            "     Stars: 0\n",
            "  3. ID: Zp9iVKa-6EgxREBtDhDkBg\n",
            "     Preview: Was here opening week. Loved the white jackets servers were wearing. The space itself was modern and clean. Not mind blowing, but elegant. Wine menu w...\n",
            "     Stars: 0\n",
            "\n",
            "DETAILED_ANALYSIS Results:\n",
            "  1. ID: WUiaWPD7C9fmdAfhmk-QZw\n",
            "     Preview: RPM struggles in one sense. It can't decide if it's a small dish (ie tapas style) type of place or if it's just a normal high end restaurant. The plat...\n",
            "     Stars: 0\n",
            "  2. ID: U6ieGW7vgpiPZMZODrQRaw\n",
            "     Preview: I heard this place was OK and then ended up loving it! After seeing the show, I had to try Mama DePandi's Bucatini Pomodoro (even though I don't typic...\n",
            "     Stars: 0\n",
            "  3. ID: Zp9iVKa-6EgxREBtDhDkBg\n",
            "     Preview: Was here opening week. Loved the white jackets servers were wearing. The space itself was modern and clean. Not mind blowing, but elegant. Wine menu w...\n",
            "     Stars: 0\n",
            "\n",
            "Query: 'mashed potatoes'\n",
            "--------------------------------------------------\n",
            "\n",
            "ZERO_SHOT Results:\n",
            "  1. ID: erhoC7CP8KTsURyo0Dq6eQ\n",
            "     Preview: AMAZING! Great cocktails. I love the pasta. My favorites are the maine lobster ravioli, truffled gnocchi, and prosciutto tortellini. Garlic whipped po...\n",
            "     Stars: 0\n",
            "  2. ID: U_JreUiWJpqEtpTao8mqBA\n",
            "     Preview: I have been to RPM three times now while visiting Chicago, and it is one of my favorite places.  Great bar scene, friendly staff and excellent food.  ...\n",
            "     Stars: 0\n",
            "  3. ID: ylkgbFWvvxXIrp_NMlOW3g\n",
            "     Preview: This place is amazing!  The staff is very attentive and the food is out of this world!! Don't miss the meatballs, short rib pappardelle or the potato ...\n",
            "     Stars: 0\n",
            "\n",
            "INSTRUCTION_BASED Results:\n",
            "  1. ID: erhoC7CP8KTsURyo0Dq6eQ\n",
            "     Preview: AMAZING! Great cocktails. I love the pasta. My favorites are the maine lobster ravioli, truffled gnocchi, and prosciutto tortellini. Garlic whipped po...\n",
            "     Stars: 0\n",
            "  2. ID: U_JreUiWJpqEtpTao8mqBA\n",
            "     Preview: I have been to RPM three times now while visiting Chicago, and it is one of my favorite places.  Great bar scene, friendly staff and excellent food.  ...\n",
            "     Stars: 0\n",
            "  3. ID: ylkgbFWvvxXIrp_NMlOW3g\n",
            "     Preview: This place is amazing!  The staff is very attentive and the food is out of this world!! Don't miss the meatballs, short rib pappardelle or the potato ...\n",
            "     Stars: 0\n",
            "\n",
            "DETAILED_ANALYSIS Results:\n",
            "  1. ID: erhoC7CP8KTsURyo0Dq6eQ\n",
            "     Preview: AMAZING! Great cocktails. I love the pasta. My favorites are the maine lobster ravioli, truffled gnocchi, and prosciutto tortellini. Garlic whipped po...\n",
            "     Stars: 0\n",
            "  2. ID: U_JreUiWJpqEtpTao8mqBA\n",
            "     Preview: I have been to RPM three times now while visiting Chicago, and it is one of my favorite places.  Great bar scene, friendly staff and excellent food.  ...\n",
            "     Stars: 0\n",
            "  3. ID: ylkgbFWvvxXIrp_NMlOW3g\n",
            "     Preview: This place is amazing!  The staff is very attentive and the food is out of this world!! Don't miss the meatballs, short rib pappardelle or the potato ...\n",
            "     Stars: 0\n",
            "\n",
            "Query: 'Grilled Shrimp Salad'\n",
            "--------------------------------------------------\n",
            "\n",
            "ZERO_SHOT Results:\n",
            "  1. ID: HuACkycdLUNsquqfrSwjNg\n",
            "     Preview: Everything we ordered here was delish! Giuliana's Italian salad and Mediterranean octopus to start: we couldn't stop raving how amazing they both were...\n",
            "     Stars: 0\n",
            "  2. ID: mGwM8Djg6KmiZeKRiGnvnQ\n",
            "     Preview: I really loved this place! I made Saturday night reservations online and went with a couple of my girlfriends. The three of us shared a bottle of Pino...\n",
            "     Stars: 0\n",
            "  3. ID: q1iJuca2tXcGl22RAv42KA\n",
            "     Preview: Met up for a late dinner with a few girlfriends after work. We walked in and was lucky enough to find a table at the bar even though the wait for a ta...\n",
            "     Stars: 0\n",
            "\n",
            "INSTRUCTION_BASED Results:\n",
            "  1. ID: HuACkycdLUNsquqfrSwjNg\n",
            "     Preview: Everything we ordered here was delish! Giuliana's Italian salad and Mediterranean octopus to start: we couldn't stop raving how amazing they both were...\n",
            "     Stars: 0\n",
            "  2. ID: mGwM8Djg6KmiZeKRiGnvnQ\n",
            "     Preview: I really loved this place! I made Saturday night reservations online and went with a couple of my girlfriends. The three of us shared a bottle of Pino...\n",
            "     Stars: 0\n",
            "  3. ID: q1iJuca2tXcGl22RAv42KA\n",
            "     Preview: Met up for a late dinner with a few girlfriends after work. We walked in and was lucky enough to find a table at the bar even though the wait for a ta...\n",
            "     Stars: 0\n",
            "\n",
            "DETAILED_ANALYSIS Results:\n",
            "  1. ID: HuACkycdLUNsquqfrSwjNg\n",
            "     Preview: Everything we ordered here was delish! Giuliana's Italian salad and Mediterranean octopus to start: we couldn't stop raving how amazing they both were...\n",
            "     Stars: 0\n",
            "  2. ID: mGwM8Djg6KmiZeKRiGnvnQ\n",
            "     Preview: I really loved this place! I made Saturday night reservations online and went with a couple of my girlfriends. The three of us shared a bottle of Pino...\n",
            "     Stars: 0\n",
            "  3. ID: q1iJuca2tXcGl22RAv42KA\n",
            "     Preview: Met up for a late dinner with a few girlfriends after work. We walked in and was lucky enough to find a table at the bar even though the wait for a ta...\n",
            "     Stars: 0\n",
            "\n",
            "Query: 'lamb Shank'\n",
            "--------------------------------------------------\n",
            "\n",
            "ZERO_SHOT Results:\n",
            "  1. ID: WUiaWPD7C9fmdAfhmk-QZw\n",
            "     Preview: RPM struggles in one sense. It can't decide if it's a small dish (ie tapas style) type of place or if it's just a normal high end restaurant. The plat...\n",
            "     Stars: 0\n",
            "  2. ID: U6ieGW7vgpiPZMZODrQRaw\n",
            "     Preview: I heard this place was OK and then ended up loving it! After seeing the show, I had to try Mama DePandi's Bucatini Pomodoro (even though I don't typic...\n",
            "     Stars: 0\n",
            "  3. ID: Zp9iVKa-6EgxREBtDhDkBg\n",
            "     Preview: Was here opening week. Loved the white jackets servers were wearing. The space itself was modern and clean. Not mind blowing, but elegant. Wine menu w...\n",
            "     Stars: 0\n",
            "\n",
            "INSTRUCTION_BASED Results:\n",
            "  1. ID: WUiaWPD7C9fmdAfhmk-QZw\n",
            "     Preview: RPM struggles in one sense. It can't decide if it's a small dish (ie tapas style) type of place or if it's just a normal high end restaurant. The plat...\n",
            "     Stars: 0\n",
            "  2. ID: U6ieGW7vgpiPZMZODrQRaw\n",
            "     Preview: I heard this place was OK and then ended up loving it! After seeing the show, I had to try Mama DePandi's Bucatini Pomodoro (even though I don't typic...\n",
            "     Stars: 0\n",
            "  3. ID: Zp9iVKa-6EgxREBtDhDkBg\n",
            "     Preview: Was here opening week. Loved the white jackets servers were wearing. The space itself was modern and clean. Not mind blowing, but elegant. Wine menu w...\n",
            "     Stars: 0\n",
            "\n",
            "DETAILED_ANALYSIS Results:\n",
            "  1. ID: WUiaWPD7C9fmdAfhmk-QZw\n",
            "     Preview: RPM struggles in one sense. It can't decide if it's a small dish (ie tapas style) type of place or if it's just a normal high end restaurant. The plat...\n",
            "     Stars: 0\n",
            "  2. ID: U6ieGW7vgpiPZMZODrQRaw\n",
            "     Preview: I heard this place was OK and then ended up loving it! After seeing the show, I had to try Mama DePandi's Bucatini Pomodoro (even though I don't typic...\n",
            "     Stars: 0\n",
            "  3. ID: Zp9iVKa-6EgxREBtDhDkBg\n",
            "     Preview: Was here opening week. Loved the white jackets servers were wearing. The space itself was modern and clean. Not mind blowing, but elegant. Wine menu w...\n",
            "     Stars: 0\n",
            "\n",
            "Query: 'Pepperoni pizza'\n",
            "--------------------------------------------------\n",
            "\n",
            "ZERO_SHOT Results:\n",
            "  1. ID: AvxVlBdhKtV1ZaBFXTYChA\n",
            "     Preview: My husband and I made it in for dinner for our anniversary.  Absolutely amazing!! To star things off we had the pepperoni pizza which was very good.  ...\n",
            "     Stars: 0\n",
            "  2. ID: A6Hadnz8TaJrPLJ70N4R2g\n",
            "     Preview: Food:  We tried their pizza, salad, spaghetti, garlic bread and spicy ink pasta.  Pizza was super good.  Everything else was also very tasty.Decor:  F...\n",
            "     Stars: 0\n",
            "  3. ID: RQhm_AIxYV-lQd3famtgsQ\n",
            "     Preview: We stopped in rpm on a Thursday around 5:30 for appetizers, or rather small plates as we must call them lately, and wine. We made reservations through...\n",
            "     Stars: 0\n",
            "\n",
            "INSTRUCTION_BASED Results:\n",
            "  1. ID: AvxVlBdhKtV1ZaBFXTYChA\n",
            "     Preview: My husband and I made it in for dinner for our anniversary.  Absolutely amazing!! To star things off we had the pepperoni pizza which was very good.  ...\n",
            "     Stars: 0\n",
            "  2. ID: A6Hadnz8TaJrPLJ70N4R2g\n",
            "     Preview: Food:  We tried their pizza, salad, spaghetti, garlic bread and spicy ink pasta.  Pizza was super good.  Everything else was also very tasty.Decor:  F...\n",
            "     Stars: 0\n",
            "  3. ID: RQhm_AIxYV-lQd3famtgsQ\n",
            "     Preview: We stopped in rpm on a Thursday around 5:30 for appetizers, or rather small plates as we must call them lately, and wine. We made reservations through...\n",
            "     Stars: 0\n",
            "\n",
            "DETAILED_ANALYSIS Results:\n",
            "  1. ID: AvxVlBdhKtV1ZaBFXTYChA\n",
            "     Preview: My husband and I made it in for dinner for our anniversary.  Absolutely amazing!! To star things off we had the pepperoni pizza which was very good.  ...\n",
            "     Stars: 0\n",
            "  2. ID: A6Hadnz8TaJrPLJ70N4R2g\n",
            "     Preview: Food:  We tried their pizza, salad, spaghetti, garlic bread and spicy ink pasta.  Pizza was super good.  Everything else was also very tasty.Decor:  F...\n",
            "     Stars: 0\n",
            "  3. ID: RQhm_AIxYV-lQd3famtgsQ\n",
            "     Preview: We stopped in rpm on a Thursday around 5:30 for appetizers, or rather small plates as we must call them lately, and wine. We made reservations through...\n",
            "     Stars: 0\n",
            "\n",
            "Query: 'brussel sprout salad'\n",
            "--------------------------------------------------\n",
            "\n",
            "ZERO_SHOT Results:\n",
            "  1. ID: rsLQPvcBsJR1ztA3VpARPw\n",
            "     Preview: I went with a friend and we ordered the burrata, tuna carpaccio, brussel sprout salad, and Mediterranean octopus.  Other than the shredded brussel spr...\n",
            "     Stars: 0\n",
            "  2. ID: fHas60-vCPrtnk7SE5-2VQ\n",
            "     Preview: I really love RPM! I went here the other night with 2 friends for dinner.  We had a reservation but arrived 20 or so minutes early to have a drink at ...\n",
            "     Stars: 0\n",
            "  3. ID: 1dGHjDcWJFQrl-1EyxujOQ\n",
            "     Preview: Some of the best Italian food I've had! I had the shredded brussels sprouts salad and truffled garlic bread. The salad was so fresh - it had the perfe...\n",
            "     Stars: 0\n",
            "\n",
            "INSTRUCTION_BASED Results:\n",
            "  1. ID: rsLQPvcBsJR1ztA3VpARPw\n",
            "     Preview: I went with a friend and we ordered the burrata, tuna carpaccio, brussel sprout salad, and Mediterranean octopus.  Other than the shredded brussel spr...\n",
            "     Stars: 0\n",
            "  2. ID: fHas60-vCPrtnk7SE5-2VQ\n",
            "     Preview: I really love RPM! I went here the other night with 2 friends for dinner.  We had a reservation but arrived 20 or so minutes early to have a drink at ...\n",
            "     Stars: 0\n",
            "  3. ID: 1dGHjDcWJFQrl-1EyxujOQ\n",
            "     Preview: Some of the best Italian food I've had! I had the shredded brussels sprouts salad and truffled garlic bread. The salad was so fresh - it had the perfe...\n",
            "     Stars: 0\n",
            "\n",
            "DETAILED_ANALYSIS Results:\n",
            "  1. ID: rsLQPvcBsJR1ztA3VpARPw\n",
            "     Preview: I went with a friend and we ordered the burrata, tuna carpaccio, brussel sprout salad, and Mediterranean octopus.  Other than the shredded brussel spr...\n",
            "     Stars: 0\n",
            "  2. ID: fHas60-vCPrtnk7SE5-2VQ\n",
            "     Preview: I really love RPM! I went here the other night with 2 friends for dinner.  We had a reservation but arrived 20 or so minutes early to have a drink at ...\n",
            "     Stars: 0\n",
            "  3. ID: 1dGHjDcWJFQrl-1EyxujOQ\n",
            "     Preview: Some of the best Italian food I've had! I had the shredded brussels sprouts salad and truffled garlic bread. The salad was so fresh - it had the perfe...\n",
            "     Stars: 0\n",
            "\n",
            "Query: 'FRIENDLY STAFF'\n",
            "--------------------------------------------------\n",
            "\n",
            "ZERO_SHOT Results:\n",
            "  1. ID: PV74AXAafSV3CYH8TF2BPQ\n",
            "     Preview: This place is terrific.  Great energy and friendly service.  With a $1000 tab I would have loved for our server to have known ounce portions of items ...\n",
            "     Stars: 0\n",
            "  2. ID: ho1wi0wccD4VNpifIt1Z4w\n",
            "     Preview: Great Food and very nice staff!\n",
            "     Stars: 0\n",
            "  3. ID: 1Lrt-DqBoncAyJFdYkJ2xw\n",
            "     Preview: Absolutely hands down the best Italian food I've ever had, friendly staff and reasonably priced for the atmosphere. Must check out!\n",
            "     Stars: 0\n",
            "\n",
            "INSTRUCTION_BASED Results:\n",
            "  1. ID: PV74AXAafSV3CYH8TF2BPQ\n",
            "     Preview: This place is terrific.  Great energy and friendly service.  With a $1000 tab I would have loved for our server to have known ounce portions of items ...\n",
            "     Stars: 0\n",
            "  2. ID: ho1wi0wccD4VNpifIt1Z4w\n",
            "     Preview: Great Food and very nice staff!\n",
            "     Stars: 0\n",
            "  3. ID: 1Lrt-DqBoncAyJFdYkJ2xw\n",
            "     Preview: Absolutely hands down the best Italian food I've ever had, friendly staff and reasonably priced for the atmosphere. Must check out!\n",
            "     Stars: 0\n",
            "\n",
            "DETAILED_ANALYSIS Results:\n",
            "  1. ID: PV74AXAafSV3CYH8TF2BPQ\n",
            "     Preview: This place is terrific.  Great energy and friendly service.  With a $1000 tab I would have loved for our server to have known ounce portions of items ...\n",
            "     Stars: 0\n",
            "  2. ID: ho1wi0wccD4VNpifIt1Z4w\n",
            "     Preview: Great Food and very nice staff!\n",
            "     Stars: 0\n",
            "  3. ID: 1Lrt-DqBoncAyJFdYkJ2xw\n",
            "     Preview: Absolutely hands down the best Italian food I've ever had, friendly staff and reasonably priced for the atmosphere. Must check out!\n",
            "     Stars: 0\n",
            "\n",
            "Query: 'Grilled Cheese'\n",
            "--------------------------------------------------\n",
            "\n",
            "ZERO_SHOT Results:\n",
            "  1. ID: d7-dBa7uzGM3QxpJMXb1Yw\n",
            "     Preview: Restaurant ReviewAnother pharm dinner in a private room. Luckily I did not have to foot the bill for this meal as it would be expensive for the amount...\n",
            "     Stars: 0\n",
            "  2. ID: q1iJuca2tXcGl22RAv42KA\n",
            "     Preview: Met up for a late dinner with a few girlfriends after work. We walked in and was lucky enough to find a table at the bar even though the wait for a ta...\n",
            "     Stars: 0\n",
            "  3. ID: HuACkycdLUNsquqfrSwjNg\n",
            "     Preview: Everything we ordered here was delish! Giuliana's Italian salad and Mediterranean octopus to start: we couldn't stop raving how amazing they both were...\n",
            "     Stars: 0\n",
            "\n",
            "INSTRUCTION_BASED Results:\n",
            "  1. ID: d7-dBa7uzGM3QxpJMXb1Yw\n",
            "     Preview: Restaurant ReviewAnother pharm dinner in a private room. Luckily I did not have to foot the bill for this meal as it would be expensive for the amount...\n",
            "     Stars: 0\n",
            "  2. ID: q1iJuca2tXcGl22RAv42KA\n",
            "     Preview: Met up for a late dinner with a few girlfriends after work. We walked in and was lucky enough to find a table at the bar even though the wait for a ta...\n",
            "     Stars: 0\n",
            "  3. ID: HuACkycdLUNsquqfrSwjNg\n",
            "     Preview: Everything we ordered here was delish! Giuliana's Italian salad and Mediterranean octopus to start: we couldn't stop raving how amazing they both were...\n",
            "     Stars: 0\n",
            "\n",
            "DETAILED_ANALYSIS Results:\n",
            "  1. ID: d7-dBa7uzGM3QxpJMXb1Yw\n",
            "     Preview: Restaurant ReviewAnother pharm dinner in a private room. Luckily I did not have to foot the bill for this meal as it would be expensive for the amount...\n",
            "     Stars: 0\n",
            "  2. ID: q1iJuca2tXcGl22RAv42KA\n",
            "     Preview: Met up for a late dinner with a few girlfriends after work. We walked in and was lucky enough to find a table at the bar even though the wait for a ta...\n",
            "     Stars: 0\n",
            "  3. ID: HuACkycdLUNsquqfrSwjNg\n",
            "     Preview: Everything we ordered here was delish! Giuliana's Italian salad and Mediterranean octopus to start: we couldn't stop raving how amazing they both were...\n",
            "     Stars: 0\n",
            "\n",
            "=== Strategy Consistency Analysis ===\n",
            "\n",
            "Query 'general chicken' - Strategy overlaps:\n",
            "  zero_shot vs instruction_based: 3.0/3 documents overlap\n",
            "  zero_shot vs detailed_analysis: 3.0/3 documents overlap\n",
            "  instruction_based vs detailed_analysis: 3.0/3 documents overlap\n",
            "\n",
            "Query 'fried chicken' - Strategy overlaps:\n",
            "  zero_shot vs instruction_based: 3.0/3 documents overlap\n",
            "  zero_shot vs detailed_analysis: 3.0/3 documents overlap\n",
            "  instruction_based vs detailed_analysis: 3.0/3 documents overlap\n",
            "\n",
            "Query 'BBQ sandwiches' - Strategy overlaps:\n",
            "  zero_shot vs instruction_based: 3.0/3 documents overlap\n",
            "  zero_shot vs detailed_analysis: 3.0/3 documents overlap\n",
            "  instruction_based vs detailed_analysis: 3.0/3 documents overlap\n",
            "\n",
            "Query 'mashed potatoes' - Strategy overlaps:\n",
            "  zero_shot vs instruction_based: 3.0/3 documents overlap\n",
            "  zero_shot vs detailed_analysis: 3.0/3 documents overlap\n",
            "  instruction_based vs detailed_analysis: 3.0/3 documents overlap\n",
            "\n",
            "Query 'Grilled Shrimp Salad' - Strategy overlaps:\n",
            "  zero_shot vs instruction_based: 3.0/3 documents overlap\n",
            "  zero_shot vs detailed_analysis: 3.0/3 documents overlap\n",
            "  instruction_based vs detailed_analysis: 3.0/3 documents overlap\n",
            "\n",
            "Query 'lamb Shank' - Strategy overlaps:\n",
            "  zero_shot vs instruction_based: 3.0/3 documents overlap\n",
            "  zero_shot vs detailed_analysis: 3.0/3 documents overlap\n",
            "  instruction_based vs detailed_analysis: 3.0/3 documents overlap\n",
            "\n",
            "Query 'Pepperoni pizza' - Strategy overlaps:\n",
            "  zero_shot vs instruction_based: 3.0/3 documents overlap\n",
            "  zero_shot vs detailed_analysis: 3.0/3 documents overlap\n",
            "  instruction_based vs detailed_analysis: 3.0/3 documents overlap\n",
            "\n",
            "Query 'brussel sprout salad' - Strategy overlaps:\n",
            "  zero_shot vs instruction_based: 3.0/3 documents overlap\n",
            "  zero_shot vs detailed_analysis: 3.0/3 documents overlap\n",
            "  instruction_based vs detailed_analysis: 3.0/3 documents overlap\n",
            "\n",
            "Query 'FRIENDLY STAFF' - Strategy overlaps:\n",
            "  zero_shot vs instruction_based: 3.0/3 documents overlap\n",
            "  zero_shot vs detailed_analysis: 3.0/3 documents overlap\n",
            "  instruction_based vs detailed_analysis: 3.0/3 documents overlap\n",
            "\n",
            "Query 'Grilled Cheese' - Strategy overlaps:\n",
            "  zero_shot vs instruction_based: 3.0/3 documents overlap\n",
            "  zero_shot vs detailed_analysis: 3.0/3 documents overlap\n",
            "  instruction_based vs detailed_analysis: 3.0/3 documents overlap\n",
            "\n",
            "Results saved to 'llm_retrieval_results.json'\n",
            "\n",
            "=== Experiment Complete ===\n",
            "✓ Tested 10 queries\n",
            "✓ Used 3 different prompt strategies\n",
            "✓ Applied consistent preprocessing from previous tasks\n",
            "✓ Implemented re-ranking approach with TF-IDF candidates\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementing pointwise, pairwise, and/or listwise prompts. Retrieving the top 3 documents for each query."
      ],
      "metadata": {
        "id": "2HMp7xuv0OPI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import time\n",
        "import re\n",
        "import glob\n",
        "import string\n",
        "import nltk\n",
        "import warnings\n",
        "from typing import List, Dict, Tuple\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "import itertools\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Download required NLTK data\n",
        "try:\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "except:\n",
        "    print(\"Note: NLTK downloads may have failed, using fallback tokenization\")\n",
        "\n",
        "# Initialize preprocessing tools\n",
        "try:\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "except:\n",
        "    stop_words = {'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours'}\n",
        "\n",
        "punctuations = set(string.punctuation)\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def normalize_token(token):\n",
        "    \"\"\"Normalize a token by converting to lowercase and handling special cases.\"\"\"\n",
        "    token = token.lower()\n",
        "    if token in punctuations:\n",
        "        return None\n",
        "    if token.replace('.', '', 1).isdigit():\n",
        "        return 'NUM'\n",
        "    return token\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Preprocess text by tokenizing, normalizing, removing stopwords, and stemming.\"\"\"\n",
        "    if not text or not isinstance(text, str):\n",
        "        return []\n",
        "\n",
        "    try:\n",
        "        tokens = word_tokenize(text)\n",
        "        processed = []\n",
        "        for token in tokens:\n",
        "            norm = normalize_token(token)\n",
        "            if norm is None or norm in stop_words:\n",
        "                continue\n",
        "            stemmed = stemmer.stem(norm)\n",
        "            processed.append(stemmed)\n",
        "        return processed\n",
        "    except LookupError:\n",
        "        simple_tokens = text.split()\n",
        "        processed = []\n",
        "        for token in simple_tokens:\n",
        "            norm = normalize_token(token)\n",
        "            if norm is None or norm in stop_words:\n",
        "                continue\n",
        "            stemmed = stemmer.stem(norm)\n",
        "            processed.append(stemmed)\n",
        "        return processed\n",
        "\n",
        "print(\"Advanced LLM Retrieval: Pointwise, Pairwise, and Listwise Approaches\")\n",
        "\n",
        "# **IMPORTANT:** Set your OpenRouter API key here\n",
        "os.environ['OPENROUTER_API_KEY'] = \"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\"\n",
        "\n",
        "# Initialize OpenRouter client\n",
        "client = openai.OpenAI(\n",
        "    base_url=\"https://openrouter.ai/api/v1\",\n",
        "    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
        ")\n",
        "\n",
        "# Choose a free model\n",
        "CHOSEN_LLM_MODEL = \"qwen/qwen3-8b:free\"\n",
        "\n",
        "def test_api_connection():\n",
        "    \"\"\"Test the OpenRouter API connection\"\"\"\n",
        "    print(\"\\n=== Testing API Connection ===\")\n",
        "    try:\n",
        "        completion = client.chat.completions.create(\n",
        "            model=CHOSEN_LLM_MODEL,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n",
        "            ],\n",
        "            max_tokens=10,\n",
        "            temperature=0.1\n",
        "        )\n",
        "        print(f\"✓ API Connection Successful!\")\n",
        "        print(f\"Test Response: {completion.choices[0].message.content}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Error connecting to OpenRouter: {e}\")\n",
        "        return False\n",
        "\n",
        "def find_yelp_files(directory_path: str) -> List[str]:\n",
        "    \"\"\"Find Yelp data files in the directory\"\"\"\n",
        "    print(f\"\\n=== Scanning directory: {directory_path} ===\")\n",
        "\n",
        "    patterns = [\n",
        "        \"yelp_academic_dataset_review.json\",\n",
        "        \"review.json\",\n",
        "        \"*review*.json\",\n",
        "        \"*.json\"\n",
        "    ]\n",
        "\n",
        "    found_files = []\n",
        "    for pattern in patterns:\n",
        "        full_pattern = os.path.join(directory_path, pattern)\n",
        "        matches = glob.glob(full_pattern)\n",
        "        if matches:\n",
        "            found_files.extend(matches)\n",
        "            break\n",
        "\n",
        "    if found_files:\n",
        "        print(f\"Found files: {found_files[:3]}...\")  # Show first 3\n",
        "    else:\n",
        "        print(f\"No JSON files found in {directory_path}\")\n",
        "        try:\n",
        "            all_files = os.listdir(directory_path)\n",
        "            print(f\"All files in directory: {all_files}\")\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    return found_files\n",
        "\n",
        "def load_yelp_data(file_path: str) -> List[Dict]:\n",
        "    \"\"\"Load Yelp reviews from JSON file with robust parsing\"\"\"\n",
        "    print(f\"\\n=== Loading Yelp Data from {file_path} ===\")\n",
        "\n",
        "    if os.path.isdir(file_path):\n",
        "        print(f\"Path is a directory. Searching for review files...\")\n",
        "        found_files = find_yelp_files(file_path)\n",
        "        if not found_files:\n",
        "            return []\n",
        "        file_path = found_files[0]\n",
        "        print(f\"Using file: {file_path}\")\n",
        "\n",
        "    try:\n",
        "        documents = []\n",
        "        total_reviews = 0\n",
        "\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            try:\n",
        "                f.seek(0)\n",
        "                data = json.load(f)\n",
        "\n",
        "                reviews = []\n",
        "                if isinstance(data, dict) and 'Reviews' in data:\n",
        "                    reviews = data['Reviews']\n",
        "                    print(f\"Found {len(reviews)} reviews in a single JSON object\")\n",
        "                elif isinstance(data, list):\n",
        "                    for obj in data:\n",
        "                        if isinstance(obj, dict):\n",
        "                            if 'Reviews' in obj:\n",
        "                                reviews.extend(obj['Reviews'])\n",
        "                            elif 'text' in obj or 'review_text' in obj:\n",
        "                                reviews.append(obj)\n",
        "                    print(f\"Found {len(reviews)} total reviews\")\n",
        "                elif isinstance(data, dict) and ('text' in data or 'review_text' in data):\n",
        "                    reviews = [data]\n",
        "\n",
        "                if not reviews:\n",
        "                    print(\"No reviews found in JSON structure!\")\n",
        "                    return []\n",
        "\n",
        "                potential_text_fields = ['text', 'Content', 'ReviewText', 'Text', 'review_text', 'comment', 'content', 'review']\n",
        "                potential_id_fields = ['review_id', 'ReviewID', 'ReviewId', 'id', 'Id', 'ID', 'reviewId', 'Review_ID']\n",
        "\n",
        "                review_field_name = None\n",
        "                review_id_field = None\n",
        "\n",
        "                for field in potential_text_fields:\n",
        "                    if field in reviews[0]:\n",
        "                        review_field_name = field\n",
        "                        print(f\"Found review text in field: '{field}'\")\n",
        "                        break\n",
        "\n",
        "                for field in potential_id_fields:\n",
        "                    if field in reviews[0]:\n",
        "                        review_id_field = field\n",
        "                        print(f\"Found review ID in field: '{field}'\")\n",
        "                        break\n",
        "\n",
        "                if not review_field_name:\n",
        "                    print(\"Could not identify review text field. Available fields:\")\n",
        "                    print(list(reviews[0].keys()) if reviews else \"No reviews found\")\n",
        "                    return []\n",
        "\n",
        "                for i, review in enumerate(reviews):\n",
        "                    if i >= 1000:  # Limit for efficiency\n",
        "                        break\n",
        "\n",
        "                    total_reviews += 1\n",
        "                    text = review.get(review_field_name, '')\n",
        "\n",
        "                    if not text or not isinstance(text, str):\n",
        "                        continue\n",
        "\n",
        "                    processed_tokens = preprocess_text(text)\n",
        "\n",
        "                    if processed_tokens:\n",
        "                        if review_id_field and review.get(review_id_field):\n",
        "                            review_id = str(review.get(review_id_field))\n",
        "                        else:\n",
        "                            review_id = f\"review_{i}\"\n",
        "\n",
        "                        documents.append({\n",
        "                            'review_id': review_id,\n",
        "                            'text': text,\n",
        "                            'processed_text': ' '.join(processed_tokens),\n",
        "                            'tokens': processed_tokens,\n",
        "                            'stars': review.get('stars', 0),\n",
        "                            'business_id': review.get('business_id', '')\n",
        "                        })\n",
        "\n",
        "            except json.JSONDecodeError:\n",
        "                print(\"Single JSON parsing failed, trying line-by-line parsing...\")\n",
        "                f.seek(0)\n",
        "                for line_num, line in enumerate(f):\n",
        "                    if line_num >= 1000:\n",
        "                        break\n",
        "                    try:\n",
        "                        line = line.strip()\n",
        "                        if not line:\n",
        "                            continue\n",
        "                        review = json.loads(line)\n",
        "\n",
        "                        text = review.get('text', '')\n",
        "                        if not text:\n",
        "                            continue\n",
        "\n",
        "                        processed_tokens = preprocess_text(text)\n",
        "\n",
        "                        if processed_tokens:\n",
        "                            documents.append({\n",
        "                                'review_id': review.get('review_id', f'review_{line_num}'),\n",
        "                                'text': text,\n",
        "                                'processed_text': ' '.join(processed_tokens),\n",
        "                                'tokens': processed_tokens,\n",
        "                                'stars': review.get('stars', 0),\n",
        "                                'business_id': review.get('business_id', '')\n",
        "                            })\n",
        "                        total_reviews += 1\n",
        "\n",
        "                    except json.JSONDecodeError:\n",
        "                        continue\n",
        "                    except Exception as e:\n",
        "                        print(f\"Skipping line {line_num}: {e}\")\n",
        "                        continue\n",
        "\n",
        "        print(f\"✓ Processed {total_reviews} reviews\")\n",
        "        print(f\"✓ Loaded {len(documents)} valid documents with preprocessing\")\n",
        "        return documents\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"✗ File not found: {file_path}\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Error loading data: {e}\")\n",
        "        return []\n",
        "\n",
        "class AdvancedLLMRetriever:\n",
        "    \"\"\"Advanced LLM-based document retrieval with pointwise, pairwise, and listwise approaches\"\"\"\n",
        "\n",
        "    def __init__(self, api_key: str = None, model_name: str = CHOSEN_LLM_MODEL):\n",
        "        self.client = client\n",
        "        self.model_name = model_name\n",
        "        self.request_delay = 2  # Increased delay for more API calls\n",
        "\n",
        "    def get_candidates_tfidf(self, query: str, documents: List[Dict], top_k: int = 5) -> Tuple[List[str], List[str], List[float]]:\n",
        "        \"\"\"Get candidate documents using TF-IDF for initial filtering\"\"\"\n",
        "        print(f\"Getting TF-IDF candidates for query: '{query}'\")\n",
        "\n",
        "        query_tokens = preprocess_text(query)\n",
        "        processed_query = ' '.join(query_tokens)\n",
        "        print(f\"Processed query: '{processed_query}'\")\n",
        "\n",
        "        processed_texts = [doc['processed_text'] for doc in documents]\n",
        "        doc_ids = [doc['review_id'] for doc in documents]\n",
        "\n",
        "        vectorizer = TfidfVectorizer(\n",
        "            stop_words=None,\n",
        "            max_features=1000,\n",
        "            ngram_range=(1, 2),\n",
        "            min_df=2,\n",
        "            lowercase=False\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            doc_vectors = vectorizer.fit_transform(processed_texts)\n",
        "            query_vector = vectorizer.transform([processed_query])\n",
        "\n",
        "            similarities = cosine_similarity(query_vector, doc_vectors).flatten()\n",
        "            top_indices = np.argsort(similarities)[::-1][:top_k]\n",
        "\n",
        "            candidate_docs = [documents[i]['text'] for i in top_indices]\n",
        "            candidate_ids = [doc_ids[i] for i in top_indices]\n",
        "            candidate_scores = [similarities[i] for i in top_indices]\n",
        "\n",
        "            print(f\"Selected {len(candidate_docs)} candidates\")\n",
        "            top_sims = [f\"{similarities[i]:.4f}\" for i in top_indices[:3]]\n",
        "            print(f\"Top similarities: {top_sims}\")\n",
        "            return candidate_ids, candidate_docs, candidate_scores\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"TF-IDF error: {e}, falling back to first {top_k} documents\")\n",
        "            return (doc_ids[:top_k],\n",
        "                   [doc['text'] for doc in documents[:top_k]],\n",
        "                   [0.0] * top_k)\n",
        "\n",
        "    def pointwise_rerank(self, query: str, candidate_docs: List[str], candidate_ids: List[str]) -> List[Tuple[str, float]]:\n",
        "        \"\"\"\n",
        "        Pointwise approach: Score each document individually for relevance to the query\n",
        "        Returns: List of (doc_id, relevance_score) tuples\n",
        "        \"\"\"\n",
        "        print(f\"Pointwise re-ranking for query: '{query}'\")\n",
        "\n",
        "        scored_docs = []\n",
        "\n",
        "        for i, (doc_id, doc_text) in enumerate(zip(candidate_ids, candidate_docs)):\n",
        "            truncated_doc = doc_text[:400] + \"...\" if len(doc_text) > 400 else doc_text\n",
        "\n",
        "            prompt = f\"\"\"Rate the relevance of this restaurant review to the query \"{query}\" on a scale of 1-10 (where 10 is highly relevant and 1 is not relevant).\n",
        "\n",
        "Review: {truncated_doc}\n",
        "\n",
        "Provide only a single number (1-10) as your response:\"\"\"\n",
        "\n",
        "            try:\n",
        "                time.sleep(self.request_delay)\n",
        "\n",
        "                response = self.client.chat.completions.create(\n",
        "                    model=self.model_name,\n",
        "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                    max_tokens=10,\n",
        "                    temperature=0.1\n",
        "                )\n",
        "\n",
        "                score_text = response.choices[0].message.content.strip()\n",
        "\n",
        "                # Extract score\n",
        "                score_match = re.search(r'\\b([1-9]|10)\\b', score_text)\n",
        "                if score_match:\n",
        "                    score = float(score_match.group(1))\n",
        "                else:\n",
        "                    score = 5.0  # Default middle score\n",
        "\n",
        "                scored_docs.append((doc_id, score))\n",
        "                print(f\"  Doc {i+1}: Score {score}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Pointwise error for doc {i+1}: {e}\")\n",
        "                scored_docs.append((doc_id, 5.0))  # Default score\n",
        "\n",
        "        # Sort by score descending\n",
        "        scored_docs.sort(key=lambda x: x[1], reverse=True)\n",
        "        return scored_docs\n",
        "\n",
        "    def pairwise_rerank(self, query: str, candidate_docs: List[str], candidate_ids: List[str]) -> List[str]:\n",
        "        \"\"\"\n",
        "        Pairwise approach: Compare documents in pairs to determine relative ranking\n",
        "        Returns: List of doc_ids in ranked order\n",
        "        \"\"\"\n",
        "        print(f\"Pairwise re-ranking for query: '{query}'\")\n",
        "\n",
        "        n_docs = len(candidate_docs)\n",
        "        if n_docs < 2:\n",
        "            return candidate_ids\n",
        "\n",
        "        # Create pairwise comparison matrix\n",
        "        wins = {doc_id: 0 for doc_id in candidate_ids}\n",
        "        comparisons = 0\n",
        "\n",
        "        # Compare each pair of documents\n",
        "        for i in range(n_docs):\n",
        "            for j in range(i + 1, n_docs):\n",
        "                doc1_id, doc1_text = candidate_ids[i], candidate_docs[i]\n",
        "                doc2_id, doc2_text = candidate_ids[j], candidate_docs[j]\n",
        "\n",
        "                # Truncate documents\n",
        "                doc1_truncated = doc1_text[:300] + \"...\" if len(doc1_text) > 300 else doc1_text\n",
        "                doc2_truncated = doc2_text[:300] + \"...\" if len(doc2_text) > 300 else doc2_text\n",
        "\n",
        "                prompt = f\"\"\"Given the query \"{query}\", which of these two restaurant reviews is more relevant?\n",
        "\n",
        "Review A: {doc1_truncated}\n",
        "\n",
        "Review B: {doc2_truncated}\n",
        "\n",
        "Respond with only \"A\" or \"B\":\"\"\"\n",
        "\n",
        "                try:\n",
        "                    time.sleep(self.request_delay)\n",
        "\n",
        "                    response = self.client.chat.completions.create(\n",
        "                        model=self.model_name,\n",
        "                        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                        max_tokens=5,\n",
        "                        temperature=0.1\n",
        "                    )\n",
        "\n",
        "                    choice = response.choices[0].message.content.strip().upper()\n",
        "\n",
        "                    if choice == 'A':\n",
        "                        wins[doc1_id] += 1\n",
        "                        print(f\"  {doc1_id} beats {doc2_id}\")\n",
        "                    elif choice == 'B':\n",
        "                        wins[doc2_id] += 1\n",
        "                        print(f\"  {doc2_id} beats {doc1_id}\")\n",
        "                    else:\n",
        "                        # Tie or unclear response\n",
        "                        wins[doc1_id] += 0.5\n",
        "                        wins[doc2_id] += 0.5\n",
        "                        print(f\"  Tie between {doc1_id} and {doc2_id}\")\n",
        "\n",
        "                    comparisons += 1\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Pairwise comparison error: {e}\")\n",
        "                    # Default to no preference\n",
        "                    wins[doc1_id] += 0.5\n",
        "                    wins[doc2_id] += 0.5\n",
        "\n",
        "        # Sort by number of wins\n",
        "        ranked_docs = sorted(wins.items(), key=lambda x: x[1], reverse=True)\n",
        "        return [doc_id for doc_id, _ in ranked_docs]\n",
        "\n",
        "    def listwise_rerank(self, query: str, candidate_docs: List[str], candidate_ids: List[str]) -> List[str]:\n",
        "        \"\"\"\n",
        "        Listwise approach: Rank all documents simultaneously\n",
        "        Returns: List of doc_ids in ranked order\n",
        "        \"\"\"\n",
        "        print(f\"Listwise re-ranking for query: '{query}'\")\n",
        "\n",
        "        # Build document list for prompt\n",
        "        doc_list = \"\"\n",
        "        for i, (doc_id, doc_text) in enumerate(zip(candidate_ids, candidate_docs)):\n",
        "            truncated_doc = doc_text[:250] + \"...\" if len(doc_text) > 250 else doc_text\n",
        "            doc_list += f\"{i+1}. (ID: {doc_id}) {truncated_doc}\\n\\n\"\n",
        "\n",
        "        prompt = f\"\"\"Rank the following restaurant reviews from most relevant to least relevant for the query: \"{query}\"\n",
        "\n",
        "{doc_list}\n",
        "\n",
        "Provide your ranking as a comma-separated list of numbers (e.g., \"3,1,5,2,4\"):\"\"\"\n",
        "\n",
        "        try:\n",
        "            time.sleep(self.request_delay)\n",
        "\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=self.model_name,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                max_tokens=50,\n",
        "                temperature=0.1\n",
        "            )\n",
        "\n",
        "            ranking_text = response.choices[0].message.content.strip()\n",
        "            print(f\"LLM ranking response: {ranking_text}\")\n",
        "\n",
        "            # Parse ranking\n",
        "            numbers = re.findall(r'\\b([1-9])\\b', ranking_text)\n",
        "\n",
        "            if len(numbers) >= len(candidate_ids):\n",
        "                # Map numbers to document IDs\n",
        "                ranked_ids = []\n",
        "                for num_str in numbers[:len(candidate_ids)]:\n",
        "                    try:\n",
        "                        idx = int(num_str) - 1  # Convert to 0-based\n",
        "                        if 0 <= idx < len(candidate_ids):\n",
        "                            if candidate_ids[idx] not in ranked_ids:\n",
        "                                ranked_ids.append(candidate_ids[idx])\n",
        "                    except ValueError:\n",
        "                        continue\n",
        "\n",
        "                # Add any missing documents\n",
        "                for doc_id in candidate_ids:\n",
        "                    if doc_id not in ranked_ids:\n",
        "                        ranked_ids.append(doc_id)\n",
        "\n",
        "                return ranked_ids[:len(candidate_ids)]\n",
        "            else:\n",
        "                print(\"Failed to parse ranking, using original order\")\n",
        "                return candidate_ids\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Listwise ranking error: {e}\")\n",
        "            return candidate_ids\n",
        "\n",
        "def run_advanced_retrieval_experiment():\n",
        "    \"\"\"Run advanced LLM retrieval experiment with multiple approaches\"\"\"\n",
        "\n",
        "    if not test_api_connection():\n",
        "        print(\"Cannot proceed without API connection.\")\n",
        "        return None, None\n",
        "\n",
        "    # Load data\n",
        "    reviews_data = load_yelp_data(\"/content/sample_data/yelp\")\n",
        "\n",
        "    if not reviews_data:\n",
        "        print(\"No data loaded. Cannot proceed with experiment.\")\n",
        "        return None, None\n",
        "\n",
        "    print(f\"Loaded {len(reviews_data)} preprocessed documents\")\n",
        "\n",
        "    # Test queries (using subset to manage API calls)\n",
        "    test_queries = [\n",
        "        \"fried chicken\",\n",
        "        \"BBQ sandwiches\",\n",
        "        \"mashed potatoes\",\n",
        "        \"FRIENDLY STAFF\",\n",
        "        \"Grilled Cheese\"\n",
        "    ]\n",
        "\n",
        "    retriever = AdvancedLLMRetriever()\n",
        "    results = {}\n",
        "\n",
        "    print(f\"\\n=== Running Advanced LLM Retrieval Experiment ===\")\n",
        "    print(f\"Testing {len(test_queries)} queries with 3 advanced approaches\")\n",
        "\n",
        "    for query in test_queries:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Processing Query: '{query}'\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        # Get TF-IDF candidates\n",
        "        candidate_ids, candidate_docs, tfidf_scores = retriever.get_candidates_tfidf(\n",
        "            query, reviews_data, top_k=5\n",
        "        )\n",
        "\n",
        "        results[query] = {\n",
        "            'tfidf_baseline': candidate_ids[:3],\n",
        "            'tfidf_scores': tfidf_scores[:3]\n",
        "        }\n",
        "\n",
        "        # Test each advanced approach\n",
        "        approaches = ['pointwise', 'pairwise', 'listwise']\n",
        "\n",
        "        for approach in approaches:\n",
        "            print(f\"\\n--- {approach.upper()} Approach ---\")\n",
        "            try:\n",
        "                if approach == 'pointwise':\n",
        "                    scored_docs = retriever.pointwise_rerank(query, candidate_docs, candidate_ids)\n",
        "                    ranked_ids = [doc_id for doc_id, score in scored_docs[:3]]\n",
        "                    results[query][approach] = ranked_ids\n",
        "                    results[query][f'{approach}_scores'] = [score for doc_id, score in scored_docs[:3]]\n",
        "\n",
        "                elif approach == 'pairwise':\n",
        "                    ranked_ids = retriever.pairwise_rerank(query, candidate_docs, candidate_ids)\n",
        "                    results[query][approach] = ranked_ids[:3]\n",
        "\n",
        "                elif approach == 'listwise':\n",
        "                    ranked_ids = retriever.listwise_rerank(query, candidate_docs, candidate_ids)\n",
        "                    results[query][approach] = ranked_ids[:3]\n",
        "\n",
        "                print(f\"{approach.capitalize()} results: {results[query][approach]}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error with {approach}: {e}\")\n",
        "                results[query][approach] = candidate_ids[:3]  # Fallback\n",
        "\n",
        "    return results, reviews_data\n",
        "\n",
        "def analyze_advanced_results(results: Dict, reviews_data: List[Dict]):\n",
        "    \"\"\"Analyze and compare advanced retrieval results\"\"\"\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"ADVANCED LLM RETRIEVAL ANALYSIS\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    doc_lookup = {doc['review_id']: doc for doc in reviews_data}\n",
        "\n",
        "    for query, query_results in results.items():\n",
        "        print(f\"\\n🎯 Query: '{query}'\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "        approaches = ['tfidf_baseline', 'pointwise', 'pairwise', 'listwise']\n",
        "\n",
        "        for approach in approaches:\n",
        "            if approach in query_results:\n",
        "                print(f\"\\n📊 {approach.upper()} Results:\")\n",
        "                doc_ids = query_results[approach]\n",
        "\n",
        "                if f'{approach}_scores' in query_results:\n",
        "                    scores = query_results[f'{approach}_scores']\n",
        "                else:\n",
        "                    scores = [None] * len(doc_ids)\n",
        "\n",
        "                for i, (doc_id, score) in enumerate(zip(doc_ids, scores), 1):\n",
        "                    if doc_id in doc_lookup:\n",
        "                        doc = doc_lookup[doc_id]\n",
        "                        preview = doc['text'][:120] + \"...\" if len(doc['text']) > 120 else doc['text']\n",
        "                        score_str = f\" (Score: {score:.1f})\" if score is not None else \"\"\n",
        "                        print(f\"  {i}. {doc_id}{score_str}\")\n",
        "                        print(f\"     {preview}\")\n",
        "                        print(f\"     Stars: {doc.get('stars', 'N/A')}\")\n",
        "                    else:\n",
        "                        print(f\"  {i}. {doc_id} (not found)\")\n",
        "\n",
        "        # Compare approach overlaps\n",
        "        print(f\"\\n🔄 Approach Overlaps for '{query}':\")\n",
        "        for i, approach1 in enumerate(approaches):\n",
        "            for approach2 in approaches[i+1:]:\n",
        "                if approach1 in query_results and approach2 in query_results:\n",
        "                    docs1 = set(query_results[approach1])\n",
        "                    docs2 = set(query_results[approach2])\n",
        "                    overlap = len(docs1.intersection(docs2))\n",
        "                    print(f\"  {approach1} vs {approach2}: {overlap}/3 documents overlap\")\n",
        "\n",
        "def save_advanced_results(results: Dict, reviews_data: List[Dict]):\n",
        "    \"\"\"Save advanced retrieval results\"\"\"\n",
        "    output_data = {\n",
        "        'advanced_retrieval_results': results,\n",
        "        'metadata': {\n",
        "            'total_documents': len(reviews_data),\n",
        "            'queries_tested': len(results),\n",
        "            'approaches_used': ['tfidf_baseline', 'pointwise', 'pairwise', 'listwise'],\n",
        "            'model_used': CHOSEN_LLM_MODEL,\n",
        "            'description': 'Advanced LLM retrieval comparison: pointwise, pairwise, and listwise approaches'\n",
        "        }\n",
        "    }\n",
        "\n",
        "    with open('advanced_llm_retrieval_results.json', 'w') as f:\n",
        "        json.dump(output_data, f, indent=2)\n",
        "    print(f\"\\nAdvanced results saved to 'advanced_llm_retrieval_results.json'\")\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\\n🚀 Starting Advanced LLM Retrieval Experiment\")\n",
        "    print(\"Testing Pointwise, Pairwise, and Listwise approaches\")\n",
        "\n",
        "    # Run experiment\n",
        "    experiment_results, reviews_data = run_advanced_retrieval_experiment()\n",
        "\n",
        "    if experiment_results and reviews_data:\n",
        "        # Analyze results\n",
        "        analyze_advanced_results(experiment_results, reviews_data)\n",
        "\n",
        "        # Save results\n",
        "        save_advanced_results(experiment_results, reviews_data)\n",
        "\n",
        "        print(f\"\\n✅ Advanced Experiment Complete!\")\n",
        "        print(f\"✓ Tested 3 advanced LLM approaches\")\n",
        "        print(f\"✓ Compared against TF-IDF baseline\")\n",
        "        print(f\"✓ Analyzed relevance and overlap patterns\")\n",
        "\n",
        "        # Summary insights\n",
        "        print(f\"\\n💡 Key Insights:\")\n",
        "        print(\"- Pointwise: Individual relevance scoring for each document\")\n",
        "        print(\"- Pairwise: Head-to-head comparisons between document pairs\")\n",
        "        print(\"- Listwise: Simultaneous ranking of all candidates\")\n",
        "        print(\"- Compare overlap patterns to see which approaches agree/disagree\")\n",
        "\n",
        "    else:\n",
        "        print(\"❌ Advanced experiment failed. Check API connection and data.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJykrg0upn4M",
        "outputId": "bf5fa8f1-6423-4026-ee8c-4679b193d34c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Advanced LLM Retrieval: Pointwise, Pairwise, and Listwise Approaches\n",
            "\n",
            "🚀 Starting Advanced LLM Retrieval Experiment\n",
            "Testing Pointwise, Pairwise, and Listwise approaches\n",
            "\n",
            "=== Testing API Connection ===\n",
            "✓ API Connection Successful!\n",
            "Test Response: \n",
            "\n",
            "=== Loading Yelp Data from /content/sample_data/yelp ===\n",
            "Path is a directory. Searching for review files...\n",
            "\n",
            "=== Scanning directory: /content/sample_data/yelp ===\n",
            "Found files: ['/content/sample_data/yelp/dlDEuDIvZI6I0cGZy4jIYg.json', '/content/sample_data/yelp/okChDSotPCRtJlQVzlnw1Q.json', '/content/sample_data/yelp/pVEyB1BxiZkJUeoqHG3ehA.json']...\n",
            "Using file: /content/sample_data/yelp/dlDEuDIvZI6I0cGZy4jIYg.json\n",
            "Found 1045 reviews in a single JSON object\n",
            "Found review text in field: 'Content'\n",
            "Found review ID in field: 'ReviewID'\n",
            "✓ Processed 1000 reviews\n",
            "✓ Loaded 1000 valid documents with preprocessing\n",
            "Loaded 1000 preprocessed documents\n",
            "\n",
            "=== Running Advanced LLM Retrieval Experiment ===\n",
            "Testing 5 queries with 3 advanced approaches\n",
            "\n",
            "============================================================\n",
            "Processing Query: 'fried chicken'\n",
            "============================================================\n",
            "Getting TF-IDF candidates for query: 'fried chicken'\n",
            "Processed query: 'fri chicken'\n",
            "Selected 5 candidates\n",
            "Top similarities: ['0.2429', '0.2411', '0.2350']\n",
            "\n",
            "--- POINTWISE Approach ---\n",
            "Pointwise re-ranking for query: 'fried chicken'\n",
            "  Doc 1: Score 5.0\n",
            "  Doc 2: Score 5.0\n",
            "  Doc 3: Score 5.0\n",
            "  Doc 4: Score 5.0\n",
            "  Doc 5: Score 5.0\n",
            "Pointwise results: ['FhsIAy1sLba19YCC7mhCgw', 'bI597D-A9A7BiF8TpqiK6Q', '9JdMXFUPVYlokLhwoqxCXA']\n",
            "\n",
            "--- PAIRWISE Approach ---\n",
            "Pairwise re-ranking for query: 'fried chicken'\n",
            "  Tie between FhsIAy1sLba19YCC7mhCgw and bI597D-A9A7BiF8TpqiK6Q\n",
            "  Tie between FhsIAy1sLba19YCC7mhCgw and 9JdMXFUPVYlokLhwoqxCXA\n",
            "  Tie between FhsIAy1sLba19YCC7mhCgw and 47Zw_31GCB8IazoHkmkaEw\n",
            "  Tie between FhsIAy1sLba19YCC7mhCgw and M5BGT17904RzU4yK9RwPcg\n",
            "  Tie between bI597D-A9A7BiF8TpqiK6Q and 9JdMXFUPVYlokLhwoqxCXA\n",
            "  Tie between bI597D-A9A7BiF8TpqiK6Q and 47Zw_31GCB8IazoHkmkaEw\n",
            "  Tie between bI597D-A9A7BiF8TpqiK6Q and M5BGT17904RzU4yK9RwPcg\n",
            "  Tie between 9JdMXFUPVYlokLhwoqxCXA and 47Zw_31GCB8IazoHkmkaEw\n",
            "  Tie between 9JdMXFUPVYlokLhwoqxCXA and M5BGT17904RzU4yK9RwPcg\n",
            "  Tie between 47Zw_31GCB8IazoHkmkaEw and M5BGT17904RzU4yK9RwPcg\n",
            "Pairwise results: ['FhsIAy1sLba19YCC7mhCgw', 'bI597D-A9A7BiF8TpqiK6Q', '9JdMXFUPVYlokLhwoqxCXA']\n",
            "\n",
            "--- LISTWISE Approach ---\n",
            "Listwise re-ranking for query: 'fried chicken'\n",
            "LLM ranking response: \n",
            "Failed to parse ranking, using original order\n",
            "Listwise results: ['FhsIAy1sLba19YCC7mhCgw', 'bI597D-A9A7BiF8TpqiK6Q', '9JdMXFUPVYlokLhwoqxCXA']\n",
            "\n",
            "============================================================\n",
            "Processing Query: 'BBQ sandwiches'\n",
            "============================================================\n",
            "Getting TF-IDF candidates for query: 'BBQ sandwiches'\n",
            "Processed query: 'bbq sandwich'\n",
            "Selected 5 candidates\n",
            "Top similarities: ['0.0000', '0.0000', '0.0000']\n",
            "\n",
            "--- POINTWISE Approach ---\n",
            "Pointwise re-ranking for query: 'BBQ sandwiches'\n",
            "  Doc 1: Score 5.0\n",
            "  Doc 2: Score 5.0\n",
            "  Doc 3: Score 5.0\n",
            "  Doc 4: Score 5.0\n",
            "  Doc 5: Score 5.0\n",
            "Pointwise results: ['WUiaWPD7C9fmdAfhmk-QZw', 'U6ieGW7vgpiPZMZODrQRaw', 'Zp9iVKa-6EgxREBtDhDkBg']\n",
            "\n",
            "--- PAIRWISE Approach ---\n",
            "Pairwise re-ranking for query: 'BBQ sandwiches'\n",
            "  Tie between WUiaWPD7C9fmdAfhmk-QZw and U6ieGW7vgpiPZMZODrQRaw\n",
            "  Tie between WUiaWPD7C9fmdAfhmk-QZw and Zp9iVKa-6EgxREBtDhDkBg\n",
            "  Tie between WUiaWPD7C9fmdAfhmk-QZw and FIGxg8fRXHGhgUZxvuJdPA\n",
            "  Tie between WUiaWPD7C9fmdAfhmk-QZw and _amOHEM76cb04sb0y4PsVQ\n",
            "  Tie between U6ieGW7vgpiPZMZODrQRaw and Zp9iVKa-6EgxREBtDhDkBg\n",
            "  Tie between U6ieGW7vgpiPZMZODrQRaw and FIGxg8fRXHGhgUZxvuJdPA\n",
            "  Tie between U6ieGW7vgpiPZMZODrQRaw and _amOHEM76cb04sb0y4PsVQ\n",
            "  Tie between Zp9iVKa-6EgxREBtDhDkBg and FIGxg8fRXHGhgUZxvuJdPA\n",
            "  Tie between Zp9iVKa-6EgxREBtDhDkBg and _amOHEM76cb04sb0y4PsVQ\n",
            "  Tie between FIGxg8fRXHGhgUZxvuJdPA and _amOHEM76cb04sb0y4PsVQ\n",
            "Pairwise results: ['WUiaWPD7C9fmdAfhmk-QZw', 'U6ieGW7vgpiPZMZODrQRaw', 'Zp9iVKa-6EgxREBtDhDkBg']\n",
            "\n",
            "--- LISTWISE Approach ---\n",
            "Listwise re-ranking for query: 'BBQ sandwiches'\n",
            "LLM ranking response: \n",
            "Failed to parse ranking, using original order\n",
            "Listwise results: ['WUiaWPD7C9fmdAfhmk-QZw', 'U6ieGW7vgpiPZMZODrQRaw', 'Zp9iVKa-6EgxREBtDhDkBg']\n",
            "\n",
            "============================================================\n",
            "Processing Query: 'mashed potatoes'\n",
            "============================================================\n",
            "Getting TF-IDF candidates for query: 'mashed potatoes'\n",
            "Processed query: 'mash potato'\n",
            "Selected 5 candidates\n",
            "Top similarities: ['0.2718', '0.2708', '0.2680']\n",
            "\n",
            "--- POINTWISE Approach ---\n",
            "Pointwise re-ranking for query: 'mashed potatoes'\n",
            "  Doc 1: Score 5.0\n",
            "  Doc 2: Score 5.0\n",
            "  Doc 3: Score 5.0\n",
            "  Doc 4: Score 5.0\n",
            "  Doc 5: Score 5.0\n",
            "Pointwise results: ['erhoC7CP8KTsURyo0Dq6eQ', 'U_JreUiWJpqEtpTao8mqBA', 'ylkgbFWvvxXIrp_NMlOW3g']\n",
            "\n",
            "--- PAIRWISE Approach ---\n",
            "Pairwise re-ranking for query: 'mashed potatoes'\n",
            "  Tie between erhoC7CP8KTsURyo0Dq6eQ and U_JreUiWJpqEtpTao8mqBA\n",
            "  Tie between erhoC7CP8KTsURyo0Dq6eQ and ylkgbFWvvxXIrp_NMlOW3g\n",
            "  Tie between erhoC7CP8KTsURyo0Dq6eQ and Hqe8isfz5NUBWoSzMZHXhg\n",
            "  Tie between erhoC7CP8KTsURyo0Dq6eQ and BqLwEn9XjM2kw3gQKuz7BQ\n",
            "  Tie between U_JreUiWJpqEtpTao8mqBA and ylkgbFWvvxXIrp_NMlOW3g\n",
            "  Tie between U_JreUiWJpqEtpTao8mqBA and Hqe8isfz5NUBWoSzMZHXhg\n",
            "  Tie between U_JreUiWJpqEtpTao8mqBA and BqLwEn9XjM2kw3gQKuz7BQ\n",
            "  Tie between ylkgbFWvvxXIrp_NMlOW3g and Hqe8isfz5NUBWoSzMZHXhg\n",
            "  Tie between ylkgbFWvvxXIrp_NMlOW3g and BqLwEn9XjM2kw3gQKuz7BQ\n",
            "  Tie between Hqe8isfz5NUBWoSzMZHXhg and BqLwEn9XjM2kw3gQKuz7BQ\n",
            "Pairwise results: ['erhoC7CP8KTsURyo0Dq6eQ', 'U_JreUiWJpqEtpTao8mqBA', 'ylkgbFWvvxXIrp_NMlOW3g']\n",
            "\n",
            "--- LISTWISE Approach ---\n",
            "Listwise re-ranking for query: 'mashed potatoes'\n",
            "LLM ranking response: \n",
            "Failed to parse ranking, using original order\n",
            "Listwise results: ['erhoC7CP8KTsURyo0Dq6eQ', 'U_JreUiWJpqEtpTao8mqBA', 'ylkgbFWvvxXIrp_NMlOW3g']\n",
            "\n",
            "============================================================\n",
            "Processing Query: 'FRIENDLY STAFF'\n",
            "============================================================\n",
            "Getting TF-IDF candidates for query: 'FRIENDLY STAFF'\n",
            "Processed query: 'friendli staff'\n",
            "Selected 5 candidates\n",
            "Top similarities: ['0.3604', '0.3351', '0.3329']\n",
            "\n",
            "--- POINTWISE Approach ---\n",
            "Pointwise re-ranking for query: 'FRIENDLY STAFF'\n",
            "  Doc 1: Score 5.0\n",
            "Pointwise error for doc 2: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-day. Add 10 credits to unlock 1000 free model requests per day', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '50', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1749081600000'}, 'provider_name': None}}, 'user_id': 'user_2y3crIfDrqXSUuOTVySDMpVger6'}\n",
            "Pointwise error for doc 3: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-day. Add 10 credits to unlock 1000 free model requests per day', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '50', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1749081600000'}, 'provider_name': None}}, 'user_id': 'user_2y3crIfDrqXSUuOTVySDMpVger6'}\n",
            "Pointwise error for doc 4: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-day. Add 10 credits to unlock 1000 free model requests per day', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '50', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1749081600000'}, 'provider_name': None}}, 'user_id': 'user_2y3crIfDrqXSUuOTVySDMpVger6'}\n",
            "Pointwise error for doc 5: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-day. Add 10 credits to unlock 1000 free model requests per day', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '50', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1749081600000'}, 'provider_name': None}}, 'user_id': 'user_2y3crIfDrqXSUuOTVySDMpVger6'}\n",
            "Pointwise results: ['PV74AXAafSV3CYH8TF2BPQ', 'ho1wi0wccD4VNpifIt1Z4w', '1Lrt-DqBoncAyJFdYkJ2xw']\n",
            "\n",
            "--- PAIRWISE Approach ---\n",
            "Pairwise re-ranking for query: 'FRIENDLY STAFF'\n",
            "Pairwise comparison error: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-day. Add 10 credits to unlock 1000 free model requests per day', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '50', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1749081600000'}, 'provider_name': None}}, 'user_id': 'user_2y3crIfDrqXSUuOTVySDMpVger6'}\n",
            "Pairwise comparison error: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-min. ', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '20', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1749067200000'}, 'provider_name': None}}, 'user_id': 'user_2y3crIfDrqXSUuOTVySDMpVger6'}\n",
            "Pairwise comparison error: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-min. ', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '20', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1749067200000'}, 'provider_name': None}}, 'user_id': 'user_2y3crIfDrqXSUuOTVySDMpVger6'}\n",
            "Pairwise comparison error: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-min. ', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '20', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1749067200000'}, 'provider_name': None}}, 'user_id': 'user_2y3crIfDrqXSUuOTVySDMpVger6'}\n",
            "Pairwise comparison error: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-min. ', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '20', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1749067200000'}, 'provider_name': None}}, 'user_id': 'user_2y3crIfDrqXSUuOTVySDMpVger6'}\n",
            "Pairwise comparison error: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-min. ', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '20', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1749067200000'}, 'provider_name': None}}, 'user_id': 'user_2y3crIfDrqXSUuOTVySDMpVger6'}\n",
            "Pairwise comparison error: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-min. ', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '20', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1749067200000'}, 'provider_name': None}}, 'user_id': 'user_2y3crIfDrqXSUuOTVySDMpVger6'}\n",
            "Pairwise comparison error: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-day. Add 10 credits to unlock 1000 free model requests per day', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '50', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1749081600000'}, 'provider_name': None}}, 'user_id': 'user_2y3crIfDrqXSUuOTVySDMpVger6'}\n",
            "Pairwise comparison error: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-day. Add 10 credits to unlock 1000 free model requests per day', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '50', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1749081600000'}, 'provider_name': None}}, 'user_id': 'user_2y3crIfDrqXSUuOTVySDMpVger6'}\n",
            "Pairwise comparison error: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-day. Add 10 credits to unlock 1000 free model requests per day', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '50', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1749081600000'}, 'provider_name': None}}, 'user_id': 'user_2y3crIfDrqXSUuOTVySDMpVger6'}\n",
            "Pairwise results: ['PV74AXAafSV3CYH8TF2BPQ', 'ho1wi0wccD4VNpifIt1Z4w', '1Lrt-DqBoncAyJFdYkJ2xw']\n",
            "\n",
            "--- LISTWISE Approach ---\n",
            "Listwise re-ranking for query: 'FRIENDLY STAFF'\n",
            "Listwise ranking error: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-day. Add 10 credits to unlock 1000 free model requests per day', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '50', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1749081600000'}, 'provider_name': None}}, 'user_id': 'user_2y3crIfDrqXSUuOTVySDMpVger6'}\n",
            "Listwise results: ['PV74AXAafSV3CYH8TF2BPQ', 'ho1wi0wccD4VNpifIt1Z4w', '1Lrt-DqBoncAyJFdYkJ2xw']\n",
            "\n",
            "============================================================\n",
            "Processing Query: 'Grilled Cheese'\n",
            "============================================================\n",
            "Getting TF-IDF candidates for query: 'Grilled Cheese'\n",
            "Processed query: 'grill chees'\n",
            "Selected 5 candidates\n",
            "Top similarities: ['0.2558', '0.2497', '0.2101']\n",
            "\n",
            "--- POINTWISE Approach ---\n",
            "Pointwise re-ranking for query: 'Grilled Cheese'\n",
            "Pointwise error for doc 1: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-day. Add 10 credits to unlock 1000 free model requests per day', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '50', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1749081600000'}, 'provider_name': None}}, 'user_id': 'user_2y3crIfDrqXSUuOTVySDMpVger6'}\n",
            "Pointwise error for doc 2: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-day. Add 10 credits to unlock 1000 free model requests per day', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '50', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1749081600000'}, 'provider_name': None}}, 'user_id': 'user_2y3crIfDrqXSUuOTVySDMpVger6'}\n",
            "Pointwise error for doc 3: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-min. ', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '20', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1749067260000'}, 'provider_name': None}}, 'user_id': 'user_2y3crIfDrqXSUuOTVySDMpVger6'}\n",
            "Pointwise error for doc 4: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-min. ', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '20', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1749067260000'}, 'provider_name': None}}, 'user_id': 'user_2y3crIfDrqXSUuOTVySDMpVger6'}\n",
            "Pointwise error for doc 5: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-min. ', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '20', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1749067260000'}, 'provider_name': None}}, 'user_id': 'user_2y3crIfDrqXSUuOTVySDMpVger6'}\n",
            "Pointwise results: ['d7-dBa7uzGM3QxpJMXb1Yw', 'q1iJuca2tXcGl22RAv42KA', 'HuACkycdLUNsquqfrSwjNg']\n",
            "\n",
            "--- PAIRWISE Approach ---\n",
            "Pairwise re-ranking for query: 'Grilled Cheese'\n",
            "Pairwise comparison error: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-min. ', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '20', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1749067260000'}, 'provider_name': None}}, 'user_id': 'user_2y3crIfDrqXSUuOTVySDMpVger6'}\n",
            "Pairwise comparison error: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-min. ', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '20', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1749067260000'}, 'provider_name': None}}, 'user_id': 'user_2y3crIfDrqXSUuOTVySDMpVger6'}\n",
            "Pairwise comparison error: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-min. ', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '20', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1749067260000'}, 'provider_name': None}}, 'user_id': 'user_2y3crIfDrqXSUuOTVySDMpVger6'}\n",
            "Pairwise comparison error: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-min. ', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '20', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1749067260000'}, 'provider_name': None}}, 'user_id': 'user_2y3crIfDrqXSUuOTVySDMpVger6'}\n",
            "Pairwise comparison error: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-min. ', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '20', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1749067260000'}, 'provider_name': None}}, 'user_id': 'user_2y3crIfDrqXSUuOTVySDMpVger6'}\n",
            "Pairwise comparison error: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-day. Add 10 credits to unlock 1000 free model requests per day', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '50', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1749081600000'}, 'provider_name': None}}, 'user_id': 'user_2y3crIfDrqXSUuOTVySDMpVger6'}\n",
            "Pairwise comparison error: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-day. Add 10 credits to unlock 1000 free model requests per day', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '50', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1749081600000'}, 'provider_name': None}}, 'user_id': 'user_2y3crIfDrqXSUuOTVySDMpVger6'}\n",
            "Pairwise comparison error: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-day. Add 10 credits to unlock 1000 free model requests per day', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '50', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1749081600000'}, 'provider_name': None}}, 'user_id': 'user_2y3crIfDrqXSUuOTVySDMpVger6'}\n",
            "Pairwise comparison error: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-day. Add 10 credits to unlock 1000 free model requests per day', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '50', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1749081600000'}, 'provider_name': None}}, 'user_id': 'user_2y3crIfDrqXSUuOTVySDMpVger6'}\n",
            "Pairwise comparison error: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-day. Add 10 credits to unlock 1000 free model requests per day', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '50', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1749081600000'}, 'provider_name': None}}, 'user_id': 'user_2y3crIfDrqXSUuOTVySDMpVger6'}\n",
            "Pairwise results: ['d7-dBa7uzGM3QxpJMXb1Yw', 'q1iJuca2tXcGl22RAv42KA', 'HuACkycdLUNsquqfrSwjNg']\n",
            "\n",
            "--- LISTWISE Approach ---\n",
            "Listwise re-ranking for query: 'Grilled Cheese'\n",
            "Listwise ranking error: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-day. Add 10 credits to unlock 1000 free model requests per day', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '50', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1749081600000'}, 'provider_name': None}}, 'user_id': 'user_2y3crIfDrqXSUuOTVySDMpVger6'}\n",
            "Listwise results: ['d7-dBa7uzGM3QxpJMXb1Yw', 'q1iJuca2tXcGl22RAv42KA', 'HuACkycdLUNsquqfrSwjNg']\n",
            "\n",
            "================================================================================\n",
            "ADVANCED LLM RETRIEVAL ANALYSIS\n",
            "================================================================================\n",
            "\n",
            "🎯 Query: 'fried chicken'\n",
            "------------------------------------------------------------\n",
            "\n",
            "📊 TFIDF_BASELINE Results:\n",
            "  1. FhsIAy1sLba19YCC7mhCgw\n",
            "     RPM is simply awesome! We were a group of four and we shared 2 bottles of wine, the polenta, tuna tartare, Julianna's Sa...\n",
            "     Stars: 0\n",
            "  2. bI597D-A9A7BiF8TpqiK6Q\n",
            "     This place is good but not as good as I was expecting. Pasta is good. Chicken was mediocre. Had a special flatbread they...\n",
            "     Stars: 0\n",
            "  3. 9JdMXFUPVYlokLhwoqxCXA\n",
            "     This Place is awesome! The appetizers of bread that we ordered were homemade and the garlic and cheese that went with we...\n",
            "     Stars: 0\n",
            "\n",
            "📊 POINTWISE Results:\n",
            "  1. FhsIAy1sLba19YCC7mhCgw (Score: 5.0)\n",
            "     RPM is simply awesome! We were a group of four and we shared 2 bottles of wine, the polenta, tuna tartare, Julianna's Sa...\n",
            "     Stars: 0\n",
            "  2. bI597D-A9A7BiF8TpqiK6Q (Score: 5.0)\n",
            "     This place is good but not as good as I was expecting. Pasta is good. Chicken was mediocre. Had a special flatbread they...\n",
            "     Stars: 0\n",
            "  3. 9JdMXFUPVYlokLhwoqxCXA (Score: 5.0)\n",
            "     This Place is awesome! The appetizers of bread that we ordered were homemade and the garlic and cheese that went with we...\n",
            "     Stars: 0\n",
            "\n",
            "📊 PAIRWISE Results:\n",
            "  1. FhsIAy1sLba19YCC7mhCgw\n",
            "     RPM is simply awesome! We were a group of four and we shared 2 bottles of wine, the polenta, tuna tartare, Julianna's Sa...\n",
            "     Stars: 0\n",
            "  2. bI597D-A9A7BiF8TpqiK6Q\n",
            "     This place is good but not as good as I was expecting. Pasta is good. Chicken was mediocre. Had a special flatbread they...\n",
            "     Stars: 0\n",
            "  3. 9JdMXFUPVYlokLhwoqxCXA\n",
            "     This Place is awesome! The appetizers of bread that we ordered were homemade and the garlic and cheese that went with we...\n",
            "     Stars: 0\n",
            "\n",
            "📊 LISTWISE Results:\n",
            "  1. FhsIAy1sLba19YCC7mhCgw\n",
            "     RPM is simply awesome! We were a group of four and we shared 2 bottles of wine, the polenta, tuna tartare, Julianna's Sa...\n",
            "     Stars: 0\n",
            "  2. bI597D-A9A7BiF8TpqiK6Q\n",
            "     This place is good but not as good as I was expecting. Pasta is good. Chicken was mediocre. Had a special flatbread they...\n",
            "     Stars: 0\n",
            "  3. 9JdMXFUPVYlokLhwoqxCXA\n",
            "     This Place is awesome! The appetizers of bread that we ordered were homemade and the garlic and cheese that went with we...\n",
            "     Stars: 0\n",
            "\n",
            "🔄 Approach Overlaps for 'fried chicken':\n",
            "  tfidf_baseline vs pointwise: 3/3 documents overlap\n",
            "  tfidf_baseline vs pairwise: 3/3 documents overlap\n",
            "  tfidf_baseline vs listwise: 3/3 documents overlap\n",
            "  pointwise vs pairwise: 3/3 documents overlap\n",
            "  pointwise vs listwise: 3/3 documents overlap\n",
            "  pairwise vs listwise: 3/3 documents overlap\n",
            "\n",
            "🎯 Query: 'BBQ sandwiches'\n",
            "------------------------------------------------------------\n",
            "\n",
            "📊 TFIDF_BASELINE Results:\n",
            "  1. WUiaWPD7C9fmdAfhmk-QZw\n",
            "     RPM struggles in one sense. It can't decide if it's a small dish (ie tapas style) type of place or if it's just a normal...\n",
            "     Stars: 0\n",
            "  2. U6ieGW7vgpiPZMZODrQRaw\n",
            "     I heard this place was OK and then ended up loving it! After seeing the show, I had to try Mama DePandi's Bucatini Pomod...\n",
            "     Stars: 0\n",
            "  3. Zp9iVKa-6EgxREBtDhDkBg\n",
            "     Was here opening week. Loved the white jackets servers were wearing. The space itself was modern and clean. Not mind blo...\n",
            "     Stars: 0\n",
            "\n",
            "📊 POINTWISE Results:\n",
            "  1. WUiaWPD7C9fmdAfhmk-QZw (Score: 5.0)\n",
            "     RPM struggles in one sense. It can't decide if it's a small dish (ie tapas style) type of place or if it's just a normal...\n",
            "     Stars: 0\n",
            "  2. U6ieGW7vgpiPZMZODrQRaw (Score: 5.0)\n",
            "     I heard this place was OK and then ended up loving it! After seeing the show, I had to try Mama DePandi's Bucatini Pomod...\n",
            "     Stars: 0\n",
            "  3. Zp9iVKa-6EgxREBtDhDkBg (Score: 5.0)\n",
            "     Was here opening week. Loved the white jackets servers were wearing. The space itself was modern and clean. Not mind blo...\n",
            "     Stars: 0\n",
            "\n",
            "📊 PAIRWISE Results:\n",
            "  1. WUiaWPD7C9fmdAfhmk-QZw\n",
            "     RPM struggles in one sense. It can't decide if it's a small dish (ie tapas style) type of place or if it's just a normal...\n",
            "     Stars: 0\n",
            "  2. U6ieGW7vgpiPZMZODrQRaw\n",
            "     I heard this place was OK and then ended up loving it! After seeing the show, I had to try Mama DePandi's Bucatini Pomod...\n",
            "     Stars: 0\n",
            "  3. Zp9iVKa-6EgxREBtDhDkBg\n",
            "     Was here opening week. Loved the white jackets servers were wearing. The space itself was modern and clean. Not mind blo...\n",
            "     Stars: 0\n",
            "\n",
            "📊 LISTWISE Results:\n",
            "  1. WUiaWPD7C9fmdAfhmk-QZw\n",
            "     RPM struggles in one sense. It can't decide if it's a small dish (ie tapas style) type of place or if it's just a normal...\n",
            "     Stars: 0\n",
            "  2. U6ieGW7vgpiPZMZODrQRaw\n",
            "     I heard this place was OK and then ended up loving it! After seeing the show, I had to try Mama DePandi's Bucatini Pomod...\n",
            "     Stars: 0\n",
            "  3. Zp9iVKa-6EgxREBtDhDkBg\n",
            "     Was here opening week. Loved the white jackets servers were wearing. The space itself was modern and clean. Not mind blo...\n",
            "     Stars: 0\n",
            "\n",
            "🔄 Approach Overlaps for 'BBQ sandwiches':\n",
            "  tfidf_baseline vs pointwise: 3/3 documents overlap\n",
            "  tfidf_baseline vs pairwise: 3/3 documents overlap\n",
            "  tfidf_baseline vs listwise: 3/3 documents overlap\n",
            "  pointwise vs pairwise: 3/3 documents overlap\n",
            "  pointwise vs listwise: 3/3 documents overlap\n",
            "  pairwise vs listwise: 3/3 documents overlap\n",
            "\n",
            "🎯 Query: 'mashed potatoes'\n",
            "------------------------------------------------------------\n",
            "\n",
            "📊 TFIDF_BASELINE Results:\n",
            "  1. erhoC7CP8KTsURyo0Dq6eQ\n",
            "     AMAZING! Great cocktails. I love the pasta. My favorites are the maine lobster ravioli, truffled gnocchi, and prosciutto...\n",
            "     Stars: 0\n",
            "  2. U_JreUiWJpqEtpTao8mqBA\n",
            "     I have been to RPM three times now while visiting Chicago, and it is one of my favorite places.  Great bar scene, friend...\n",
            "     Stars: 0\n",
            "  3. ylkgbFWvvxXIrp_NMlOW3g\n",
            "     This place is amazing!  The staff is very attentive and the food is out of this world!! Don't miss the meatballs, short ...\n",
            "     Stars: 0\n",
            "\n",
            "📊 POINTWISE Results:\n",
            "  1. erhoC7CP8KTsURyo0Dq6eQ (Score: 5.0)\n",
            "     AMAZING! Great cocktails. I love the pasta. My favorites are the maine lobster ravioli, truffled gnocchi, and prosciutto...\n",
            "     Stars: 0\n",
            "  2. U_JreUiWJpqEtpTao8mqBA (Score: 5.0)\n",
            "     I have been to RPM three times now while visiting Chicago, and it is one of my favorite places.  Great bar scene, friend...\n",
            "     Stars: 0\n",
            "  3. ylkgbFWvvxXIrp_NMlOW3g (Score: 5.0)\n",
            "     This place is amazing!  The staff is very attentive and the food is out of this world!! Don't miss the meatballs, short ...\n",
            "     Stars: 0\n",
            "\n",
            "📊 PAIRWISE Results:\n",
            "  1. erhoC7CP8KTsURyo0Dq6eQ\n",
            "     AMAZING! Great cocktails. I love the pasta. My favorites are the maine lobster ravioli, truffled gnocchi, and prosciutto...\n",
            "     Stars: 0\n",
            "  2. U_JreUiWJpqEtpTao8mqBA\n",
            "     I have been to RPM three times now while visiting Chicago, and it is one of my favorite places.  Great bar scene, friend...\n",
            "     Stars: 0\n",
            "  3. ylkgbFWvvxXIrp_NMlOW3g\n",
            "     This place is amazing!  The staff is very attentive and the food is out of this world!! Don't miss the meatballs, short ...\n",
            "     Stars: 0\n",
            "\n",
            "📊 LISTWISE Results:\n",
            "  1. erhoC7CP8KTsURyo0Dq6eQ\n",
            "     AMAZING! Great cocktails. I love the pasta. My favorites are the maine lobster ravioli, truffled gnocchi, and prosciutto...\n",
            "     Stars: 0\n",
            "  2. U_JreUiWJpqEtpTao8mqBA\n",
            "     I have been to RPM three times now while visiting Chicago, and it is one of my favorite places.  Great bar scene, friend...\n",
            "     Stars: 0\n",
            "  3. ylkgbFWvvxXIrp_NMlOW3g\n",
            "     This place is amazing!  The staff is very attentive and the food is out of this world!! Don't miss the meatballs, short ...\n",
            "     Stars: 0\n",
            "\n",
            "🔄 Approach Overlaps for 'mashed potatoes':\n",
            "  tfidf_baseline vs pointwise: 3/3 documents overlap\n",
            "  tfidf_baseline vs pairwise: 3/3 documents overlap\n",
            "  tfidf_baseline vs listwise: 3/3 documents overlap\n",
            "  pointwise vs pairwise: 3/3 documents overlap\n",
            "  pointwise vs listwise: 3/3 documents overlap\n",
            "  pairwise vs listwise: 3/3 documents overlap\n",
            "\n",
            "🎯 Query: 'FRIENDLY STAFF'\n",
            "------------------------------------------------------------\n",
            "\n",
            "📊 TFIDF_BASELINE Results:\n",
            "  1. PV74AXAafSV3CYH8TF2BPQ\n",
            "     This place is terrific.  Great energy and friendly service.  With a $1000 tab I would have loved for our server to have ...\n",
            "     Stars: 0\n",
            "  2. ho1wi0wccD4VNpifIt1Z4w\n",
            "     Great Food and very nice staff!\n",
            "     Stars: 0\n",
            "  3. 1Lrt-DqBoncAyJFdYkJ2xw\n",
            "     Absolutely hands down the best Italian food I've ever had, friendly staff and reasonably priced for the atmosphere. Must...\n",
            "     Stars: 0\n",
            "\n",
            "📊 POINTWISE Results:\n",
            "  1. PV74AXAafSV3CYH8TF2BPQ (Score: 5.0)\n",
            "     This place is terrific.  Great energy and friendly service.  With a $1000 tab I would have loved for our server to have ...\n",
            "     Stars: 0\n",
            "  2. ho1wi0wccD4VNpifIt1Z4w (Score: 5.0)\n",
            "     Great Food and very nice staff!\n",
            "     Stars: 0\n",
            "  3. 1Lrt-DqBoncAyJFdYkJ2xw (Score: 5.0)\n",
            "     Absolutely hands down the best Italian food I've ever had, friendly staff and reasonably priced for the atmosphere. Must...\n",
            "     Stars: 0\n",
            "\n",
            "📊 PAIRWISE Results:\n",
            "  1. PV74AXAafSV3CYH8TF2BPQ\n",
            "     This place is terrific.  Great energy and friendly service.  With a $1000 tab I would have loved for our server to have ...\n",
            "     Stars: 0\n",
            "  2. ho1wi0wccD4VNpifIt1Z4w\n",
            "     Great Food and very nice staff!\n",
            "     Stars: 0\n",
            "  3. 1Lrt-DqBoncAyJFdYkJ2xw\n",
            "     Absolutely hands down the best Italian food I've ever had, friendly staff and reasonably priced for the atmosphere. Must...\n",
            "     Stars: 0\n",
            "\n",
            "📊 LISTWISE Results:\n",
            "  1. PV74AXAafSV3CYH8TF2BPQ\n",
            "     This place is terrific.  Great energy and friendly service.  With a $1000 tab I would have loved for our server to have ...\n",
            "     Stars: 0\n",
            "  2. ho1wi0wccD4VNpifIt1Z4w\n",
            "     Great Food and very nice staff!\n",
            "     Stars: 0\n",
            "  3. 1Lrt-DqBoncAyJFdYkJ2xw\n",
            "     Absolutely hands down the best Italian food I've ever had, friendly staff and reasonably priced for the atmosphere. Must...\n",
            "     Stars: 0\n",
            "\n",
            "🔄 Approach Overlaps for 'FRIENDLY STAFF':\n",
            "  tfidf_baseline vs pointwise: 3/3 documents overlap\n",
            "  tfidf_baseline vs pairwise: 3/3 documents overlap\n",
            "  tfidf_baseline vs listwise: 3/3 documents overlap\n",
            "  pointwise vs pairwise: 3/3 documents overlap\n",
            "  pointwise vs listwise: 3/3 documents overlap\n",
            "  pairwise vs listwise: 3/3 documents overlap\n",
            "\n",
            "🎯 Query: 'Grilled Cheese'\n",
            "------------------------------------------------------------\n",
            "\n",
            "📊 TFIDF_BASELINE Results:\n",
            "  1. d7-dBa7uzGM3QxpJMXb1Yw\n",
            "     Restaurant ReviewAnother pharm dinner in a private room. Luckily I did not have to foot the bill for this meal as it wou...\n",
            "     Stars: 0\n",
            "  2. q1iJuca2tXcGl22RAv42KA\n",
            "     Met up for a late dinner with a few girlfriends after work. We walked in and was lucky enough to find a table at the bar...\n",
            "     Stars: 0\n",
            "  3. HuACkycdLUNsquqfrSwjNg\n",
            "     Everything we ordered here was delish! Giuliana's Italian salad and Mediterranean octopus to start: we couldn't stop rav...\n",
            "     Stars: 0\n",
            "\n",
            "📊 POINTWISE Results:\n",
            "  1. d7-dBa7uzGM3QxpJMXb1Yw (Score: 5.0)\n",
            "     Restaurant ReviewAnother pharm dinner in a private room. Luckily I did not have to foot the bill for this meal as it wou...\n",
            "     Stars: 0\n",
            "  2. q1iJuca2tXcGl22RAv42KA (Score: 5.0)\n",
            "     Met up for a late dinner with a few girlfriends after work. We walked in and was lucky enough to find a table at the bar...\n",
            "     Stars: 0\n",
            "  3. HuACkycdLUNsquqfrSwjNg (Score: 5.0)\n",
            "     Everything we ordered here was delish! Giuliana's Italian salad and Mediterranean octopus to start: we couldn't stop rav...\n",
            "     Stars: 0\n",
            "\n",
            "📊 PAIRWISE Results:\n",
            "  1. d7-dBa7uzGM3QxpJMXb1Yw\n",
            "     Restaurant ReviewAnother pharm dinner in a private room. Luckily I did not have to foot the bill for this meal as it wou...\n",
            "     Stars: 0\n",
            "  2. q1iJuca2tXcGl22RAv42KA\n",
            "     Met up for a late dinner with a few girlfriends after work. We walked in and was lucky enough to find a table at the bar...\n",
            "     Stars: 0\n",
            "  3. HuACkycdLUNsquqfrSwjNg\n",
            "     Everything we ordered here was delish! Giuliana's Italian salad and Mediterranean octopus to start: we couldn't stop rav...\n",
            "     Stars: 0\n",
            "\n",
            "📊 LISTWISE Results:\n",
            "  1. d7-dBa7uzGM3QxpJMXb1Yw\n",
            "     Restaurant ReviewAnother pharm dinner in a private room. Luckily I did not have to foot the bill for this meal as it wou...\n",
            "     Stars: 0\n",
            "  2. q1iJuca2tXcGl22RAv42KA\n",
            "     Met up for a late dinner with a few girlfriends after work. We walked in and was lucky enough to find a table at the bar...\n",
            "     Stars: 0\n",
            "  3. HuACkycdLUNsquqfrSwjNg\n",
            "     Everything we ordered here was delish! Giuliana's Italian salad and Mediterranean octopus to start: we couldn't stop rav...\n",
            "     Stars: 0\n",
            "\n",
            "🔄 Approach Overlaps for 'Grilled Cheese':\n",
            "  tfidf_baseline vs pointwise: 3/3 documents overlap\n",
            "  tfidf_baseline vs pairwise: 3/3 documents overlap\n",
            "  tfidf_baseline vs listwise: 3/3 documents overlap\n",
            "  pointwise vs pairwise: 3/3 documents overlap\n",
            "  pointwise vs listwise: 3/3 documents overlap\n",
            "  pairwise vs listwise: 3/3 documents overlap\n",
            "\n",
            "Advanced results saved to 'advanced_llm_retrieval_results.json'\n",
            "\n",
            "✅ Advanced Experiment Complete!\n",
            "✓ Tested 3 advanced LLM approaches\n",
            "✓ Compared against TF-IDF baseline\n",
            "✓ Analyzed relevance and overlap patterns\n",
            "\n",
            "💡 Key Insights:\n",
            "- Pointwise: Individual relevance scoring for each document\n",
            "- Pairwise: Head-to-head comparisons between document pairs\n",
            "- Listwise: Simultaneous ranking of all candidates\n",
            "- Compare overlap patterns to see which approaches agree/disagree\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qhdaNaUkXH_"
      },
      "source": [
        "# Extra Credits (10pts)\n",
        "\n",
        "Further explore LLM-based retrieval. For example, you may consider pointwise, pairwise, and/or listwise prompts. Retrieve the top 3 documents for each query. Did you find the documents more relevant than documents retrieved by ?\n",
        "\n",
        "\n",
        "# Submission\n",
        "\n",
        "This assignment has in total 100 points. The deadline is June 4th 23:59 PDT. You should submit your report in **PDF** using the homework latex template, and submit your code (notebook)."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}